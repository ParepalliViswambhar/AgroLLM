{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-04T06:08:28.166886Z",
     "iopub.status.busy": "2025-11-04T06:08:28.166680Z",
     "iopub.status.idle": "2025-11-04T06:10:00.109041Z",
     "shell.execute_reply": "2025-11-04T06:10:00.108332Z",
     "shell.execute_reply.started": "2025-11-04T06:08:28.166869Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m449.8/449.8 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers accelerate langchain langchain-community faiss-cpu sentence-transformers pymupdf SpeechRecognition google-generativeai langdetect deep_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T06:10:00.110854Z",
     "iopub.status.busy": "2025-11-04T06:10:00.110630Z",
     "iopub.status.idle": "2025-11-04T06:10:00.116012Z",
     "shell.execute_reply": "2025-11-04T06:10:00.115340Z",
     "shell.execute_reply.started": "2025-11-04T06:10:00.110834Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pickle\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "# from langchain_community.llms import HuggingFacePipeline\n",
    "# from langchain.vectorstores import FAISS\n",
    "# from langchain.document_loaders import PyMuPDFLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.chains import RetrievalQA\n",
    "\n",
    "# # Paths\n",
    "# #DATA_FOLDER = \"/kaggle/input/agrollm-data/agrollm_dataset\"\n",
    "# DATA_FOLDER = \"/kaggle/input/agrollm-2/dataset/dataset\"\n",
    "# INDEX_DIR = \"/kaggle/working/AgroRAG/faiss_index\"\n",
    "# CHUNKS_PATH = \"/kaggle/working/AgroRAG/chunks.pkl\"\n",
    "\n",
    "# # 1. Load PDFs from subfolders\n",
    "# def load_documents_from_folders(root_dir):\n",
    "#     all_docs = []\n",
    "#     for subdir, _, files in os.walk(root_dir):\n",
    "#         for file in files:\n",
    "#             if file.endswith(\".pdf\"):\n",
    "#                 path = os.path.join(subdir, file)\n",
    "#                 print(f\"Loading {path}...\")\n",
    "#                 loader = PyMuPDFLoader(path)\n",
    "#                 docs = loader.load()\n",
    "#                 for doc in docs:\n",
    "#                     doc.metadata['source'] = path\n",
    "#                     doc.metadata['topic'] = os.path.basename(subdir)\n",
    "#                 all_docs.extend(docs)\n",
    "#     return all_docs\n",
    "\n",
    "# # 2. Split text into chunks\n",
    "# def split_documents(documents, chunk_size=500, chunk_overlap=50):\n",
    "#     splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "#     return splitter.split_documents(documents)\n",
    "\n",
    "# # 3. Embed and store\n",
    "# def build_or_load_faiss_index(chunks, index_path=INDEX_DIR):\n",
    "#     embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "#     if os.path.exists(os.path.join(index_path, \"index.faiss\")):\n",
    "#         print(\"Loading existing FAISS index...\")\n",
    "#         return FAISS.load_local(index_path, embedder)\n",
    "#     print(\"Creating new FAISS index...\")\n",
    "#     vectordb = FAISS.from_documents(chunks, embedder)\n",
    "#     os.makedirs(index_path, exist_ok=True)\n",
    "#     vectordb.save_local(index_path)\n",
    "#     return vectordb\n",
    "\n",
    "# # 4. Load Mistral-7B-Instruct for Local RAG\n",
    "# def load_local_llm():\n",
    "#     model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "#     pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, temperature=0.7)\n",
    "#     return HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# # 5. RAG QA Chain with Local Model\n",
    "# def run_qa_chain(vectorstore, question):\n",
    "#     llm = load_local_llm()\n",
    "#     retriever = vectorstore.as_retriever(search_type=\"similarity\", k=4)\n",
    "#     qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "#     return qa.run(question)\n",
    "\n",
    "# # Main Pipeline\n",
    "# if __name__ == \"__main__\":\n",
    "#     if os.path.exists(CHUNKS_PATH):\n",
    "#         print(\"âœ… Loading pre-saved chunks...\")\n",
    "#         with open(CHUNKS_PATH, 'rb') as f:\n",
    "#             chunks = pickle.load(f)\n",
    "#     else:\n",
    "#         print(\"ðŸ“„ Loading and chunking documents...\")\n",
    "#         docs = load_documents_from_folders(DATA_FOLDER)\n",
    "#         chunks = split_documents(docs)\n",
    "#         os.makedirs(os.path.dirname(CHUNKS_PATH), exist_ok=True)\n",
    "#         with open(CHUNKS_PATH, 'wb') as f:\n",
    "#             pickle.dump(chunks, f)\n",
    "\n",
    "#     print(\"ðŸ§  Building or loading FAISS index...\")\n",
    "#     db = build_or_load_faiss_index(chunks)\n",
    "\n",
    "#     print(\"\\nâœ… Chunking and indexing complete. You can now use the inference script to ask questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T06:10:00.118942Z",
     "iopub.status.busy": "2025-11-04T06:10:00.118489Z",
     "iopub.status.idle": "2025-11-04T06:13:12.279709Z",
     "shell.execute_reply": "2025-11-04T06:13:12.278980Z",
     "shell.execute_reply.started": "2025-11-04T06:10:00.118924Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 06:10:16.652155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762236616.895856      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762236616.966122      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ENHANCED REAL-TIME DYNAMIC AgroRAG System...\n",
      "Initializing speech recognition...\n",
      "Initializing Gemini model...\n",
      "Loading FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37/1990434726.py:1266: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d5d66cf06847dba745463858f1e0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626e607f6d434957814c2acae0853b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5daf5fbe87f4a55b2b3745e93ed7c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb21e19ac02149038be517819051a1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11434688a4e4d19ad8dda7a63b9806f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7ea1f858e6484c9fe347fe77cbb0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aff6f159c34485bac2a34d93ff50e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf40a07e49c4a24bb8adf95cd27f383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b521df08b914508947ddc80489eb37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e2908a2df5417484d2a6bcdd254f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491e521095b34bc1be616a26ae6a73ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Mistral model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddc117c0c884dffb7ca70b518ef05fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4720784d896149b695d49ddcabdb99ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1dfd927bcd34be086c5b9cc4f129b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691213e6fdf84bad80d693daa9eebae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c32c7104cd74bc0a2706fe51689843c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45f68a9d7094f45869fb12970299054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a76b6ed3194ae4876c8d5c8b786be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d561df548947ac9bb6d49f4d4cee72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c50002e1b24d1e9d05f8fcb7bfed1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d477c707414dad83f88a39bf4cdead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c48937e10c48c293ae16479e56a1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a2ce3f766c402da74397ffb3d4130d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "/tmp/ipykernel_37/1990434726.py:1298: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ENHANCED REAL-TIME DYNAMIC AgroRAG System Ready!\n",
      "\n",
      "ðŸš€ **ALL FEATURES FROM VERSION1 + VERSION2:**\n",
      "âœ“ LLM-driven agricultural intelligence (version1)\n",
      "âœ“ Expert analysis mode (EXACTLY 400 words, 800 tokens) (version1)\n",
      "âœ“ Non-agricultural question restriction (version1)\n",
      "âœ“ Query rewriting with conversation history (version1)\n",
      "âœ“ Stricter agricultural image classification (version1)\n",
      "âœ“ Intent analysis with LLM (version1)\n",
      "âœ“ Greeting/thanks handling (version1)\n",
      "âœ“ Dynamic Intent Detection (Gemini AI) (version2)\n",
      "âœ“ \"Here\" location support with session memory (version2)\n",
      "âœ“ Location setting: \"I am in [City]\" (version2)\n",
      "âœ“ Concurrent processing (5 parallel)\n",
      "âœ“ Live weather data - 15 days (Open-Meteo API) (version2)\n",
      "âœ“ AI-powered soil analysis (Gemini AI) (version2)\n",
      "âœ“ Dynamic crop requirements (Gemini AI) (version2)\n",
      "âœ“ Crop price prediction with 3-month forecast (Gemini AI) (version2)\n",
      "âœ“ Contextual follow-up suggestions (version2)\n",
      "âœ“ Similar questions API endpoint (version2)\n",
      "âœ“ Location-based recommendations (version2)\n",
      "âœ“ Multilingual: English, Hindi, Bengali, Telugu, Marathi, Tamil, Gujarati, Kannada, Malayalam, Punjabi, Odia, Assamese, Urdu\n",
      "\n",
      "**Try asking:**\n",
      "- \"I am in Pune\" (sets location)\n",
      "- \"What is the best crop here?\" (uses saved location)\n",
      "- \"Best crops for Mumbai\" (gets weather, soil, prices)\n",
      "- \"Tell me more about it\" (rewrites with history)\n",
      "- \"Hello\" (gets friendly greeting)\n",
      "\n",
      "**API Endpoints:**\n",
      "- get_similar_questions_api(question, location, topic)\n",
      "- fetch_crop_prices(crop_name, location)\n",
      "\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* Running on public URL: https://2f7a4ad146f6535361.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2f7a4ad146f6535361.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from huggingface_hub import login\n",
    "import speech_recognition as sr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gc\n",
    "import wave\n",
    "import tempfile\n",
    "import google.generativeai as genai\n",
    "import base64\n",
    "import io\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from langdetect import detect\n",
    "from deep_translator import GoogleTranslator\n",
    "import uuid\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import queue\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import requests\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "import json\n",
    "\n",
    "# Configure Gemini API\n",
    "GEMINI_API_KEY = \"REPLACE_WITH_YOUR_GEMINI_API_KEY\"\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Weather API Configuration (using Open-Meteo - free, no key required)\n",
    "WEATHER_API_BASE = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "# CHANGED: Alternative Soil API - using World Soil Information Service (free, no auth)\n",
    "# Alternative option: Use Gemini AI to estimate soil properties based on location\n",
    "SOIL_ESTIMATION_MODE = \"gemini\"  # Options: \"gemini\" or \"static\"\n",
    "\n",
    "# Crop Price API Configuration (using mock data for now - can integrate real API)\n",
    "CROP_PRICE_API_BASE = \"REPLACE_WITH_YOUR_CROP_PRICE_API_BASE\"  # Example Indian govt API\n",
    "\n",
    "# Login to Hugging Face\n",
    "login(\"REPLACE_WITH_YOUR_HUGGING_FACE_TOKEN\")\n",
    "\n",
    "# OPTIMIZATION: Set optimal GPU settings\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Global variables\n",
    "qa_chain = None\n",
    "your_llm_pipeline = None\n",
    "recognizer = sr.Recognizer()\n",
    "gemini_model = None\n",
    "INDEX_PATH = \"/kaggle/working/AgroRAG/faiss_index\"\n",
    "geolocator = Nominatim(user_agent=\"agrorag_assistant\")\n",
    "\n",
    "# CONCURRENCY SETTINGS\n",
    "MAX_CONCURRENT_REQUESTS = 5\n",
    "REQUEST_QUEUE_SIZE = 20\n",
    "executor = ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS, thread_name_prefix=\"AgroRAG\")\n",
    "request_semaphore = threading.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "# SUPPORTED LANGUAGES\n",
    "SUPPORTED_LANGUAGES = {\n",
    "    'en': 'English', 'hi': 'Hindi', 'bn': 'Bengali', 'te': 'Telugu',\n",
    "    'mr': 'Marathi', 'ta': 'Tamil', 'gu': 'Gujarati', 'kn': 'Kannada',\n",
    "    'ml': 'Malayalam', 'pa': 'Punjabi', 'or': 'Odia', 'as': 'Assamese', 'ur': 'Urdu'\n",
    "}\n",
    "\n",
    "# DYNAMIC DATA CACHE (thread-safe)\n",
    "class DataCache:\n",
    "    def __init__(self, ttl_minutes=30):\n",
    "        self.cache = {}\n",
    "        self.lock = threading.Lock()\n",
    "        self.ttl = timedelta(minutes=ttl_minutes)\n",
    "    \n",
    "    def get(self, key):\n",
    "        with self.lock:\n",
    "            if key in self.cache:\n",
    "                data, timestamp = self.cache[key]\n",
    "                if datetime.now() - timestamp < self.ttl:\n",
    "                    return data\n",
    "                del self.cache[key]\n",
    "            return None\n",
    "    \n",
    "    def set(self, key, value):\n",
    "        with self.lock:\n",
    "            self.cache[key] = (value, datetime.now())\n",
    "    \n",
    "    def clear_old(self):\n",
    "        with self.lock:\n",
    "            current_time = datetime.now()\n",
    "            expired = [k for k, (_, ts) in self.cache.items() if current_time - ts >= self.ttl]\n",
    "            for k in expired:\n",
    "                del self.cache[k]\n",
    "\n",
    "weather_cache = DataCache(ttl_minutes=30)\n",
    "soil_cache = DataCache(ttl_minutes=60)\n",
    "crop_cache = DataCache(ttl_minutes=120)\n",
    "price_cache = DataCache(ttl_minutes=1440)  # 24 hours for price data\n",
    "\n",
    "class SessionManager:\n",
    "    def __init__(self, max_sessions=100, max_history_per_session=10):\n",
    "        self.session_histories = {}\n",
    "        self.session_lock = threading.Lock()\n",
    "        self.max_sessions = max_sessions\n",
    "        self.max_history = max_history_per_session\n",
    "        self.session_timestamps = {}\n",
    "        self.active_requests = defaultdict(int)\n",
    "        self.location_cache = {}\n",
    "    \n",
    "    def get_session_id(self, request: gr.Request = None, provided_session_id: str = None):\n",
    "        if provided_session_id and provided_session_id.strip():\n",
    "            return f\"api_{provided_session_id.strip()}\"\n",
    "        if request and hasattr(request, 'session_hash') and request.session_hash:\n",
    "            return f\"web_{request.session_hash}\"\n",
    "        return f\"temp_{str(uuid.uuid4())[:8]}\"\n",
    "    \n",
    "    def fetch_session_id(self, request: gr.Request = None):\n",
    "        if request and hasattr(request, 'session_hash') and request.session_hash:\n",
    "            session_id = f\"web_{request.session_hash}\"\n",
    "            return {\"status\": \"success\", \"session_id\": session_id, \"display_id\": session_id[-8:], \n",
    "                   \"type\": \"web_ui\", \"timestamp\": datetime.now().isoformat()}\n",
    "        new_id = f\"api_{str(uuid.uuid4())[:8]}\"\n",
    "        return {\"status\": \"success\", \"session_id\": new_id, \"display_id\": new_id[-8:], \n",
    "               \"type\": \"generated\", \"timestamp\": datetime.now().isoformat()}\n",
    "    \n",
    "    def get_history(self, session_id):\n",
    "        with self.session_lock:\n",
    "            return self.session_histories.get(session_id, [])\n",
    "    \n",
    "    def add_to_history(self, session_id, user_input, bot_response, image_context=\"\", \n",
    "                      original_lang=\"en\", is_detailed=False, question_type=\"new_question\", \n",
    "                      location=None, dynamic_data=None):\n",
    "        with self.session_lock:\n",
    "            if session_id not in self.session_histories:\n",
    "                self.session_histories[session_id] = []\n",
    "            \n",
    "            exchange = {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"user_input\": user_input,\n",
    "                \"bot_response\": bot_response,\n",
    "                \"image_context\": image_context,\n",
    "                \"language\": original_lang,\n",
    "                \"is_detailed\": is_detailed,\n",
    "                \"question_type\": question_type,\n",
    "                \"location\": location,\n",
    "                \"dynamic_data\": dynamic_data\n",
    "            }\n",
    "            \n",
    "            self.session_histories[session_id].append(exchange)\n",
    "            self.session_timestamps[session_id] = datetime.now()\n",
    "            \n",
    "            if len(self.session_histories[session_id]) > self.max_history:\n",
    "                self.session_histories[session_id] = self.session_histories[session_id][-self.max_history:]\n",
    "            \n",
    "            self._cleanup_old_sessions()\n",
    "    \n",
    "    def _cleanup_old_sessions(self):\n",
    "        if len(self.session_histories) > self.max_sessions:\n",
    "            sorted_sessions = sorted(self.session_timestamps.items(), key=lambda x: x[1], reverse=True)\n",
    "            sessions_to_keep = [sid for sid, _ in sorted_sessions[:self.max_sessions]]\n",
    "            sessions_to_remove = set(self.session_histories.keys()) - set(sessions_to_keep)\n",
    "            for sid in sessions_to_remove:\n",
    "                if sid in self.session_histories:\n",
    "                    del self.session_histories[sid]\n",
    "                if sid in self.session_timestamps:\n",
    "                    del self.session_timestamps[sid]\n",
    "    \n",
    "    def clear_session_history(self, session_id):\n",
    "        with self.session_lock:\n",
    "            if session_id in self.session_histories:\n",
    "                del self.session_histories[session_id]\n",
    "            if session_id in self.session_timestamps:\n",
    "                del self.session_timestamps[session_id]\n",
    "    \n",
    "    def increment_active_requests(self, session_id):\n",
    "        with self.session_lock:\n",
    "            self.active_requests[session_id] += 1\n",
    "    \n",
    "    def decrement_active_requests(self, session_id):\n",
    "        with self.session_lock:\n",
    "            self.active_requests[session_id] = max(0, self.active_requests[session_id] - 1)\n",
    "    \n",
    "    def get_session_stats(self):\n",
    "        with self.session_lock:\n",
    "            return {\n",
    "                \"total_sessions\": len(self.session_histories),\n",
    "                \"total_exchanges\": sum(len(h) for h in self.session_histories.values()),\n",
    "                \"active_sessions\": len([s for s, t in self.session_timestamps.items() \n",
    "                                      if (datetime.now() - t).seconds < 3600]),\n",
    "                \"active_requests\": sum(self.active_requests.values()),\n",
    "                \"max_sessions\": self.max_sessions,\n",
    "                \"max_history\": self.max_history\n",
    "            }\n",
    "    \n",
    "    def set_location(self, session_id, location):\n",
    "        with self.session_lock:\n",
    "            self.location_cache[session_id] = location\n",
    "    \n",
    "    def get_location(self, session_id):\n",
    "        with self.session_lock:\n",
    "            return self.location_cache.get(session_id)\n",
    "\n",
    "session_manager = SessionManager()\n",
    "\n",
    "# ===== DYNAMIC DATA FETCHING FUNCTIONS =====\n",
    "\n",
    "def geocode_location(location_query):\n",
    "    \"\"\"Convert location query to coordinates using Geopy\"\"\"\n",
    "    try:\n",
    "        location = geolocator.geocode(location_query, timeout=10)\n",
    "        if location:\n",
    "            return {\n",
    "                \"name\": location.address,\n",
    "                \"latitude\": location.latitude,\n",
    "                \"longitude\": location.longitude,\n",
    "                \"success\": True\n",
    "            }\n",
    "        return {\"success\": False, \"error\": \"Location not found\"}\n",
    "    except GeocoderTimedOut:\n",
    "        return {\"success\": False, \"error\": \"Geocoding timeout\"}\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "def fetch_weather_data(latitude, longitude, days=15):\n",
    "    \"\"\"UPDATED: Fetch 15-day weather forecast from Open-Meteo API\"\"\"\n",
    "    cache_key = f\"weather_{latitude}_{longitude}_{days}\"\n",
    "    cached = weather_cache.get(cache_key)\n",
    "    if cached:\n",
    "        print(f\"Using cached weather data\")\n",
    "        return cached\n",
    "    \n",
    "    try:\n",
    "        params = {\n",
    "            'latitude': latitude,\n",
    "            'longitude': longitude,\n",
    "            'daily': 'temperature_2m_max,temperature_2m_min,precipitation_sum,rain_sum,'\n",
    "                    'precipitation_probability_max,windspeed_10m_max,relative_humidity_2m_mean',\n",
    "            'forecast_days': days,  # Changed to 15 days\n",
    "            'timezone': 'auto'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(WEATHER_API_BASE, params=params, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        weather_data = {\n",
    "            \"success\": True,\n",
    "            \"location\": {\"lat\": latitude, \"lon\": longitude},\n",
    "            \"daily_forecast\": []\n",
    "        }\n",
    "        \n",
    "        daily = data.get('daily', {})\n",
    "        dates = daily.get('time', [])\n",
    "        \n",
    "        for i in range(len(dates)):\n",
    "            weather_data[\"daily_forecast\"].append({\n",
    "                \"date\": dates[i],\n",
    "                \"temp_max\": daily.get('temperature_2m_max', [])[i],\n",
    "                \"temp_min\": daily.get('temperature_2m_min', [])[i],\n",
    "                \"precipitation_mm\": daily.get('precipitation_sum', [])[i],\n",
    "                \"rain_mm\": daily.get('rain_sum', [])[i],\n",
    "                \"precipitation_prob\": daily.get('precipitation_probability_max', [])[i],\n",
    "                \"windspeed_kmh\": daily.get('windspeed_10m_max', [])[i],\n",
    "                \"humidity_percent\": daily.get('relative_humidity_2m_mean', [])[i]\n",
    "            })\n",
    "        \n",
    "        weather_cache.set(cache_key, weather_data)\n",
    "        print(f\"Fetched fresh 15-day weather data for {latitude}, {longitude}\")\n",
    "        return weather_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Weather API error: {e}\")\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "def fetch_soil_data_gemini(latitude, longitude, location_name):\n",
    "    \"\"\"NEW: Use Gemini AI to estimate soil properties based on location\"\"\"\n",
    "    cache_key = f\"soil_gemini_{latitude}_{longitude}\"\n",
    "    cached = soil_cache.get(cache_key)\n",
    "    if cached:\n",
    "        print(f\"Using cached soil data (Gemini)\")\n",
    "        return cached\n",
    "    \n",
    "    try:\n",
    "        prompt = f\"\"\"Provide typical soil characteristics for agricultural land near coordinates {latitude}, {longitude} ({location_name}).\n",
    "\n",
    "Return ONLY valid JSON with this exact structure:\n",
    "{{\n",
    "    \"ph\": 6.5,\n",
    "    \"clay_percent\": 25,\n",
    "    \"sand_percent\": 40,\n",
    "    \"silt_percent\": 35,\n",
    "    \"nitrogen_g_kg\": 1.5,\n",
    "    \"organic_carbon_g_kg\": 12,\n",
    "    \"soil_type\": \"loam\",\n",
    "    \"drainage\": \"good\",\n",
    "    \"fertility\": \"medium\"\n",
    "}}\n",
    "\n",
    "Base estimates on typical soil patterns for this region. Use realistic agricultural values.\"\"\"\n",
    "\n",
    "        response = gemini_model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.1,\n",
    "                max_output_tokens=300\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if response and response.text:\n",
    "            text = response.text.strip()\n",
    "            text = re.sub(r'```json\\s*', '', text)\n",
    "            text = re.sub(r'```\\s*$', '', text)\n",
    "            \n",
    "            soil_data = json.loads(text)\n",
    "            soil_data['success'] = True\n",
    "            soil_data['location'] = {\"lat\": latitude, \"lon\": longitude}\n",
    "            soil_data['source'] = 'gemini_estimation'\n",
    "            \n",
    "            soil_cache.set(cache_key, soil_data)\n",
    "            print(f\"Fetched soil data from Gemini for {location_name}\")\n",
    "            return soil_data\n",
    "        \n",
    "        return {\"success\": False, \"error\": \"No response from Gemini\"}\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decode error for soil: {e}\")\n",
    "        return {\"success\": False, \"error\": \"Invalid JSON response\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini soil data error: {e}\")\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "def fetch_soil_data(latitude, longitude, location_name=\"\"):\n",
    "    \"\"\"Wrapper function that uses Gemini for soil estimation\"\"\"\n",
    "    return fetch_soil_data_gemini(latitude, longitude, location_name)\n",
    "\n",
    "def get_crop_requirements_from_gemini(crop_name):\n",
    "    \"\"\"Dynamically fetch crop requirements using Gemini AI\"\"\"\n",
    "    cache_key = f\"crop_{crop_name.lower()}\"\n",
    "    cached = crop_cache.get(cache_key)\n",
    "    if cached:\n",
    "        print(f\"Using cached crop data for {crop_name}\")\n",
    "        return cached\n",
    "    \n",
    "    try:\n",
    "        prompt = f\"\"\"Provide detailed agricultural requirements for growing {crop_name} in JSON format.\n",
    "\n",
    "Return ONLY valid JSON with this exact structure:\n",
    "{{\n",
    "    \"crop_name\": \"{crop_name}\",\n",
    "    \"temperature_range_celsius\": {{\"min\": X, \"max\": Y}},\n",
    "    \"annual_rainfall_mm\": {{\"min\": X, \"max\": Y}},\n",
    "    \"soil_ph_range\": {{\"min\": X, \"max\": Y}},\n",
    "    \"suitable_soil_types\": [\"type1\", \"type2\"],\n",
    "    \"growing_season\": [\"season1\", \"season2\"],\n",
    "    \"duration_days\": X,\n",
    "    \"water_requirement\": \"low/medium/high\",\n",
    "    \"key_nutrients\": [\"N\", \"P\", \"K\"],\n",
    "    \"common_regions\": [\"region1\", \"region2\"]\n",
    "}}\n",
    "\n",
    "Be precise with numerical values. Use standard agricultural data.\"\"\"\n",
    "\n",
    "        response = gemini_model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.1,\n",
    "                max_output_tokens=500\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if response and response.text:\n",
    "            text = response.text.strip()\n",
    "            text = re.sub(r'```json\\s*', '', text)\n",
    "            text = re.sub(r'```\\s*$', '', text)\n",
    "            \n",
    "            crop_data = json.loads(text)\n",
    "            crop_cache.set(cache_key, crop_data)\n",
    "            print(f\"Fetched fresh crop data for {crop_name} from Gemini\")\n",
    "            return crop_data\n",
    "        \n",
    "        return {\"success\": False, \"error\": \"No response from Gemini\"}\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decode error for crop {crop_name}: {e}\")\n",
    "        return {\"success\": False, \"error\": \"Invalid JSON response\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini crop data error: {e}\")\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "def get_batch_crop_requirements_from_gemini(crop_names):\n",
    "    \"\"\"Fetch requirements for multiple crops in a single Gemini API call\"\"\"\n",
    "    # Check cache first\n",
    "    uncached_crops = []\n",
    "    cached_results = {}\n",
    "    \n",
    "    for crop in crop_names:\n",
    "        cache_key = f\"crop_{crop.lower()}\"\n",
    "        cached = crop_cache.get(cache_key)\n",
    "        if cached:\n",
    "            cached_results[crop] = cached\n",
    "            print(f\"Using cached crop data for {crop}\")\n",
    "        else:\n",
    "            uncached_crops.append(crop)\n",
    "    \n",
    "    # If all crops are cached, return immediately\n",
    "    if not uncached_crops:\n",
    "        return cached_results\n",
    "    \n",
    "    # Fetch uncached crops in batch\n",
    "    try:\n",
    "        crops_list = ', '.join(uncached_crops)\n",
    "        prompt = f\"\"\"Provide detailed agricultural requirements for the following crops: {crops_list}\n",
    "\n",
    "Return ONLY valid JSON with this exact structure - an object where each key is a crop name:\n",
    "{{\n",
    "    \"crop1\": {{\n",
    "        \"crop_name\": \"crop1\",\n",
    "        \"temperature_range_celsius\": {{\"min\": X, \"max\": Y}},\n",
    "        \"annual_rainfall_mm\": {{\"min\": X, \"max\": Y}},\n",
    "        \"soil_ph_range\": {{\"min\": X, \"max\": Y}},\n",
    "        \"suitable_soil_types\": [\"type1\", \"type2\"],\n",
    "        \"growing_season\": [\"season1\", \"season2\"],\n",
    "        \"duration_days\": X,\n",
    "        \"water_requirement\": \"low/medium/high\",\n",
    "        \"key_nutrients\": [\"N\", \"P\", \"K\"],\n",
    "        \"common_regions\": [\"region1\", \"region2\"]\n",
    "    }},\n",
    "    \"crop2\": {{\n",
    "        ...\n",
    "    }}\n",
    "}}\n",
    "\n",
    "Provide data for ALL crops: {crops_list}. Be precise with numerical values. Use standard agricultural data.\"\"\"\n",
    "\n",
    "        response = gemini_model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.1,\n",
    "                max_output_tokens=3000  # Increased for multiple crops\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if response and response.text:\n",
    "            text = response.text.strip()\n",
    "            text = re.sub(r'```json\\s*', '', text)\n",
    "            text = re.sub(r'```\\s*$', '', text)\n",
    "            \n",
    "            batch_data = json.loads(text)\n",
    "            \n",
    "            # Cache individual crop data\n",
    "            for crop in uncached_crops:\n",
    "                crop_lower = crop.lower()\n",
    "                # Try to find crop data with case-insensitive matching\n",
    "                crop_data = None\n",
    "                for key in batch_data.keys():\n",
    "                    if key.lower() == crop_lower:\n",
    "                        crop_data = batch_data[key]\n",
    "                        break\n",
    "                \n",
    "                if crop_data:\n",
    "                    cache_key = f\"crop_{crop_lower}\"\n",
    "                    crop_cache.set(cache_key, crop_data)\n",
    "                    cached_results[crop] = crop_data\n",
    "                    print(f\"Fetched and cached crop data for {crop}\")\n",
    "                else:\n",
    "                    print(f\"Warning: No data returned for {crop}\")\n",
    "            \n",
    "            return cached_results\n",
    "        \n",
    "        print(\"No response from Gemini for batch crop request\")\n",
    "        return cached_results  # Return whatever we have cached\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decode error for batch crops: {e}\")\n",
    "        # Fallback to individual requests\n",
    "        print(\"Falling back to individual crop requests...\")\n",
    "        for crop in uncached_crops:\n",
    "            crop_data = get_crop_requirements_from_gemini(crop)\n",
    "            if 'success' not in crop_data or crop_data.get('success') != False:\n",
    "                cached_results[crop] = crop_data\n",
    "        return cached_results\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini batch crop data error: {e}\")\n",
    "        # Fallback to individual requests\n",
    "        print(\"Falling back to individual crop requests...\")\n",
    "        for crop in uncached_crops:\n",
    "            crop_data = get_crop_requirements_from_gemini(crop)\n",
    "            if 'success' not in crop_data or crop_data.get('success') != False:\n",
    "                cached_results[crop] = crop_data\n",
    "        return cached_results\n",
    "\n",
    "def analyze_crop_suitability(weather_data, soil_data, crop_requirements):\n",
    "    \"\"\"Analyze if crop is suitable based on weather and soil\"\"\"\n",
    "    if not weather_data.get('success') or not soil_data.get('success'):\n",
    "        return {\"suitable\": False, \"reason\": \"Missing data\"}\n",
    "    \n",
    "    issues = []\n",
    "    score = 100\n",
    "    \n",
    "    # Check temperature (using 15-day data)\n",
    "    forecast_days = min(15, len(weather_data['daily_forecast']))\n",
    "    avg_temp = sum(d['temp_max'] + d['temp_min'] for d in weather_data['daily_forecast'][:forecast_days]) / (2 * forecast_days)\n",
    "    temp_req = crop_requirements.get('temperature_range_celsius', {})\n",
    "    temp_min, temp_max = temp_req.get('min', 0), temp_req.get('max', 50)\n",
    "    \n",
    "    if avg_temp < temp_min:\n",
    "        issues.append(f\"Temperature too low (avg {avg_temp:.1f}Â°C, needs {temp_min}-{temp_max}Â°C)\")\n",
    "        score -= 30\n",
    "    elif avg_temp > temp_max:\n",
    "        issues.append(f\"Temperature too high (avg {avg_temp:.1f}Â°C, needs {temp_min}-{temp_max}Â°C)\")\n",
    "        score -= 30\n",
    "    \n",
    "    # Check rainfall (project from 15-day data)\n",
    "    total_rain = sum(d['precipitation_mm'] for d in weather_data['daily_forecast'][:forecast_days])\n",
    "    projected_annual = (total_rain / forecast_days) * 365  # Project to annual\n",
    "    rain_req = crop_requirements.get('annual_rainfall_mm', {})\n",
    "    rain_min, rain_max = rain_req.get('min', 0), rain_req.get('max', 5000)\n",
    "    \n",
    "    if projected_annual < rain_min:\n",
    "        issues.append(f\"Insufficient rainfall (projected {projected_annual:.0f}mm/year, needs {rain_min}-{rain_max}mm)\")\n",
    "        score -= 25\n",
    "    elif projected_annual > rain_max:\n",
    "        issues.append(f\"Excess rainfall (projected {projected_annual:.0f}mm/year, needs {rain_min}-{rain_max}mm)\")\n",
    "        score -= 15\n",
    "    \n",
    "    # Check soil pH\n",
    "    soil_ph = soil_data.get('ph', 7.0)\n",
    "    ph_req = crop_requirements.get('soil_ph_range', {})\n",
    "    ph_min, ph_max = ph_req.get('min', 5.5), ph_req.get('max', 8.0)\n",
    "    \n",
    "    if soil_ph < ph_min or soil_ph > ph_max:\n",
    "        issues.append(f\"Soil pH {soil_ph:.1f} outside optimal range ({ph_min}-{ph_max})\")\n",
    "        score -= 20\n",
    "    \n",
    "    # Check soil type\n",
    "    soil_type = soil_data.get('soil_type', 'unknown')\n",
    "    suitable_soils = crop_requirements.get('suitable_soil_types', [])\n",
    "    if suitable_soils and soil_type not in suitable_soils:\n",
    "        issues.append(f\"Soil type '{soil_type}' not ideal (prefers {', '.join(suitable_soils)})\")\n",
    "        score -= 15\n",
    "    \n",
    "    return {\n",
    "        \"suitable\": score >= 50,\n",
    "        \"suitability_score\": max(0, score),\n",
    "        \"issues\": issues,\n",
    "        \"recommendations\": [] if score >= 70 else [\"Consider irrigation\" if projected_annual < rain_min else \"\"]\n",
    "    }\n",
    "\n",
    "def recommend_best_crops(location_query, session_id):\n",
    "    \"\"\"Main function to recommend crops based on location\"\"\"\n",
    "    \n",
    "    # Geocode location\n",
    "    location = geocode_location(location_query)\n",
    "    if not location.get('success'):\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Could not find location: {location_query}\",\n",
    "            \"suggestions\": []\n",
    "        }\n",
    "    \n",
    "    session_manager.set_location(session_id, location)\n",
    "    lat, lon = location['latitude'], location['longitude']\n",
    "    \n",
    "    # Fetch dynamic data (15-day weather + Gemini soil)\n",
    "    weather_data = fetch_weather_data(lat, lon, days=15)\n",
    "    soil_data = fetch_soil_data(lat, lon, location['name'])\n",
    "    \n",
    "    if not weather_data.get('success') or not soil_data.get('success'):\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"Could not fetch weather or soil data\",\n",
    "            \"suggestions\": []\n",
    "        }\n",
    "    \n",
    "    # Common crops to check\n",
    "    crops_to_check = ['rice', 'wheat', 'maize', 'cotton', 'sugarcane', 'soybean', \n",
    "                     'groundnut', 'tomato', 'potato', 'pulses', 'chickpea', 'mustard']\n",
    "    \n",
    "    # Fetch all crop requirements at once\n",
    "    print(f\"Fetching requirements for {len(crops_to_check)} crops in batch...\")\n",
    "    all_crop_requirements = get_batch_crop_requirements_from_gemini(crops_to_check)\n",
    "    \n",
    "    crop_analyses = []\n",
    "    \n",
    "    for crop in crops_to_check:\n",
    "        crop_req = all_crop_requirements.get(crop)\n",
    "        if not crop_req or crop_req.get('success') == False:\n",
    "            print(f\"Skipping {crop} - no valid data\")\n",
    "            continue\n",
    "        \n",
    "        analysis = analyze_crop_suitability(weather_data, soil_data, crop_req)\n",
    "        crop_analyses.append({\n",
    "            \"crop\": crop,\n",
    "            \"score\": analysis['suitability_score'],\n",
    "            \"suitable\": analysis['suitable'],\n",
    "            \"issues\": analysis['issues'],\n",
    "            \"requirements\": crop_req\n",
    "        })\n",
    "    \n",
    "    # Sort by suitability score\n",
    "    crop_analyses.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    forecast_days = len(weather_data['daily_forecast'])\n",
    "    \n",
    "    return {\n",
    "        \"success\": True,\n",
    "        \"location\": location,\n",
    "        \"weather_summary\": {\n",
    "            \"avg_temp\": sum(d['temp_max'] + d['temp_min'] for d in weather_data['daily_forecast']) / (2 * forecast_days),\n",
    "            \"total_rainfall_15d\": sum(d['precipitation_mm'] for d in weather_data['daily_forecast']),\n",
    "            \"avg_humidity\": sum(d['humidity_percent'] for d in weather_data['daily_forecast']) / forecast_days,\n",
    "            \"forecast_days\": forecast_days\n",
    "        },\n",
    "        \"soil_summary\": soil_data,\n",
    "        \"top_crops\": crop_analyses[:5],\n",
    "        \"all_analyses\": crop_analyses\n",
    "    }\n",
    "\n",
    "def fetch_crop_prices(crop_name, location_query=None):\n",
    "    \"\"\"Fetch current crop prices and predict future trends using Gemini AI\"\"\"\n",
    "    cache_key = f\"price_{crop_name.lower()}_{location_query or 'general'}\"\n",
    "    cached = price_cache.get(cache_key)\n",
    "    if cached:\n",
    "        print(f\"Using cached price data for {crop_name}\")\n",
    "        return cached\n",
    "    \n",
    "    try:\n",
    "        # Use Gemini to generate realistic price data and predictions\n",
    "        prompt = f\"\"\"Provide current market price information and 3-month price prediction for {crop_name} in India{' ('+location_query+')' if location_query else ''}.\n",
    "\n",
    "Return ONLY valid JSON with this exact structure:\n",
    "{{\n",
    "    \"crop_name\": \"{crop_name}\",\n",
    "    \"current_price_per_quintal\": {{\n",
    "        \"min\": X,\n",
    "        \"max\": Y,\n",
    "        \"average\": Z\n",
    "    }},\n",
    "    \"currency\": \"INR\",\n",
    "    \"unit\": \"quintal\",\n",
    "    \"market_trend\": \"rising/stable/falling\",\n",
    "    \"price_prediction_3months\": {{\n",
    "        \"expected_min\": X,\n",
    "        \"expected_max\": Y,\n",
    "        \"confidence\": \"high/medium/low\"\n",
    "    }},\n",
    "    \"factors_affecting_price\": [\"factor1\", \"factor2\"],\n",
    "    \"best_selling_season\": \"season_name\",\n",
    "    \"data_source\": \"market_analysis\"\n",
    "}}\n",
    "\n",
    "Use realistic Indian agricultural market prices. Base on current market conditions.\"\"\"\n",
    "\n",
    "        response = gemini_model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.1,\n",
    "                max_output_tokens=400\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if response and response.text:\n",
    "            text = response.text.strip()\n",
    "            text = re.sub(r'```json\\s*', '', text)\n",
    "            text = re.sub(r'```\\s*$', '', text)\n",
    "            \n",
    "            price_data = json.loads(text)\n",
    "            price_data['success'] = True\n",
    "            price_data['timestamp'] = datetime.now().isoformat()\n",
    "            \n",
    "            price_cache.set(cache_key, price_data)\n",
    "            print(f\"Fetched fresh price data for {crop_name}\")\n",
    "            return price_data\n",
    "        \n",
    "        return {\"success\": False, \"error\": \"No response from Gemini\"}\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decode error for prices: {e}\")\n",
    "        return {\"success\": False, \"error\": \"Invalid JSON response\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini price data error: {e}\")\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "def generate_contextual_followup_questions(question, response, context_data):\n",
    "    \"\"\"Generate 3 relevant follow-up questions using Gemini\"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"Based on this agricultural conversation, suggest 3 highly relevant follow-up questions the farmer might ask next.\n",
    "\n",
    "USER QUESTION: {question}\n",
    "ASSISTANT RESPONSE: {response[:500]}...\n",
    "CONTEXT: Location={context_data.get('location', 'unknown')}, Has weather data={context_data.get('has_weather', False)}\n",
    "\n",
    "Generate 3 practical, specific questions that:\n",
    "1. Build naturally on the conversation\n",
    "2. Are relevant to the farmer's situation\n",
    "3. Are actionable and helpful\n",
    "\n",
    "Format as:\n",
    "1. [question]\n",
    "2. [question]\n",
    "3. [question]\n",
    "\n",
    "Keep each question under 15 words.\"\"\"\n",
    "\n",
    "        result = gemini_model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.7,\n",
    "                max_output_tokens=200\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if result and result.text:\n",
    "            questions = []\n",
    "            for line in result.text.strip().split('\\n'):\n",
    "                line = line.strip()\n",
    "                if re.match(r'^\\d+\\.', line):\n",
    "                    question = re.sub(r'^\\d+\\.\\s*', '', line)\n",
    "                    questions.append(question)\n",
    "            \n",
    "            return questions[:3] if len(questions) >= 3 else questions\n",
    "        \n",
    "        return []\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Follow-up generation error: {e}\")\n",
    "        return []\n",
    "\n",
    "def detect_question_intent_with_gemini(question):\n",
    "    \"\"\"NEW: Use Gemini to dynamically detect if question is about crop recommendation\"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"Analyze this agricultural question and determine its intent.\n",
    "\n",
    "QUESTION: \"{question}\"\n",
    "\n",
    "Classify into ONE category:\n",
    "1. CROP_RECOMMENDATION - asking about best/suitable crops for a location\n",
    "2. CROP_SPECIFIC - asking about a specific crop's care/diseases/cultivation\n",
    "3. GENERAL_INFO - general agricultural knowledge question\n",
    "4. WEATHER_QUERY - asking about weather/climate\n",
    "5. SOIL_QUERY - asking about soil conditions\n",
    "6. PEST_DISEASE - asking about pests or diseases\n",
    "7. LOCATION_SETTING - user is setting/providing their location\n",
    "8. OTHER - doesn't fit above categories\n",
    "\n",
    "Respond with ONLY the category name (e.g., \"CROP_RECOMMENDATION\").\"\"\"\n",
    "\n",
    "        response = gemini_model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.1,\n",
    "                max_output_tokens=50\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if response and response.text:\n",
    "            intent = response.text.strip().upper()\n",
    "            print(f\"[Intent Detection] Question: '{question[:50]}...' -> Intent: {intent}\")\n",
    "            return intent\n",
    "        \n",
    "        return \"OTHER\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Intent detection error: {e}\")\n",
    "        return \"OTHER\"\n",
    "\n",
    "def get_similar_questions_for_query(user_question, context_data=None):\n",
    "    \"\"\"Generate similar questions that users might ask - SEPARATE ENDPOINT\"\"\"\n",
    "    try:\n",
    "        context_str = \"\"\n",
    "        if context_data:\n",
    "            context_str = f\"\\nContext: Location={context_data.get('location', 'N/A')}, Topic={context_data.get('topic', 'general agriculture')}\"\n",
    "        \n",
    "        prompt = f\"\"\"Given this agricultural question, generate 5 similar questions that farmers commonly ask on the same topic.\n",
    "\n",
    "ORIGINAL QUESTION: \"{user_question}\"{context_str}\n",
    "\n",
    "Generate 5 variations/related questions that:\n",
    "1. Are on the same agricultural topic\n",
    "2. Explore different angles of the same problem\n",
    "3. Are practical and commonly asked by farmers\n",
    "4. Vary in specificity (some general, some specific)\n",
    "\n",
    "Format as:\n",
    "1. [question]\n",
    "2. [question]\n",
    "3. [question]\n",
    "4. [question]\n",
    "5. [question]\n",
    "\n",
    "Keep each under 20 words.\"\"\"\n",
    "\n",
    "        result = gemini_model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.6,\n",
    "                max_output_tokens=300\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if result and result.text:\n",
    "            similar_questions = []\n",
    "            for line in result.text.strip().split('\\n'):\n",
    "                line = line.strip()\n",
    "                if re.match(r'^\\d+\\.', line):\n",
    "                    q = re.sub(r'^\\d+\\.\\s*', '', line)\n",
    "                    similar_questions.append(q)\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"original_question\": user_question,\n",
    "                \"similar_questions\": similar_questions[:5],\n",
    "                \"count\": len(similar_questions[:5])\n",
    "            }\n",
    "        \n",
    "        return {\"success\": False, \"error\": \"No response generated\"}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Similar questions error: {e}\")\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "# ===== EXISTING FUNCTIONS (keeping essential ones) =====\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        detected_lang = detect(text)\n",
    "        if detected_lang in SUPPORTED_LANGUAGES:\n",
    "            return detected_lang\n",
    "        return 'en'\n",
    "    except:\n",
    "        return 'en'\n",
    "\n",
    "def translate_to_english(text):\n",
    "    try:\n",
    "        detected_lang = detect_language(text)\n",
    "        if detected_lang == 'en':\n",
    "            return text, detected_lang\n",
    "        if detected_lang not in SUPPORTED_LANGUAGES:\n",
    "            return text, 'en'\n",
    "        translator = GoogleTranslator(source=detected_lang, target='en')\n",
    "        translated = translator.translate(text)\n",
    "        return translated, detected_lang\n",
    "    except:\n",
    "        return text, 'en'\n",
    "\n",
    "def translate_from_english(text, target_lang):\n",
    "    if target_lang == 'en' or target_lang == 'english':\n",
    "        return text\n",
    "    if target_lang not in SUPPORTED_LANGUAGES:\n",
    "        return text\n",
    "    try:\n",
    "        translator = GoogleTranslator(source='en', target=target_lang)\n",
    "        return translator.translate(text)\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "def clean_answer(text):\n",
    "    text = re.sub(r\"https?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\[[^\\]]*\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def cleanup_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def get_conversation_context(session_id):\n",
    "    \"\"\"Get enhanced conversation context for LLM from session history\"\"\"\n",
    "    conversation_history = session_manager.get_history(session_id)\n",
    "    \n",
    "    if not conversation_history:\n",
    "        return \"\"\n",
    "    \n",
    "    context_parts = [\"=== CONVERSATION HISTORY ===\"]\n",
    "    \n",
    "    recent_exchanges = conversation_history[-3:]\n",
    "    \n",
    "    for i, exchange in enumerate(recent_exchanges, 1):\n",
    "        timestamp = datetime.fromisoformat(exchange['timestamp']).strftime(\"%H:%M\")\n",
    "        context_parts.append(f\"Exchange {i} [{timestamp}]:\")\n",
    "        context_parts.append(f\"User: {exchange['user_input']}\")\n",
    "        \n",
    "        if exchange.get('image_context'):\n",
    "            context_parts.append(f\"Image: {exchange['image_context']}\")\n",
    "        \n",
    "        response = exchange['bot_response']\n",
    "        if len(response) > 200:\n",
    "            response = response[:200] + \"...\"\n",
    "        context_parts.append(f\"Assistant: {response}\")\n",
    "        context_parts.append(\"\")\n",
    "    \n",
    "    context_parts.append(\"=== END HISTORY ===\")\n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "def analyze_user_intent_with_llm(question, conversation_context):\n",
    "    \"\"\"LLM-based analysis with better prompting to reduce ambiguity\"\"\"\n",
    "    global your_llm_pipeline\n",
    "    \n",
    "    if your_llm_pipeline is None:\n",
    "        return {\n",
    "            'is_agricultural': True,\n",
    "            'is_greeting': False,\n",
    "            'references_history': False,\n",
    "            'interaction_type': 'question'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # REFINED INTENT PROMPT: More explicit rules\n",
    "        intent_prompt = f\"\"\"<s>[INST] You are an agricultural classification agent. Classify if user input is agriculture-related.\n",
    "\n",
    "AGRICULTURAL KEYWORDS (if ANY of these appear â†’ AGRICULTURAL: YES):\n",
    "- Farm terms: crop, plant, seed, harvest, field, farm, agriculture, cultivation\n",
    "- Animals: cattle, cow, buffalo, goat, sheep, pig, chicken, duck, fish, livestock, poultry\n",
    "- Products: milk, dairy, egg, meat, wool, honey, silk\n",
    "- Soil: fertilizer, compost, manure, soil, irrigation, water, drainage\n",
    "- Problems: disease, pest, insect, weed, drought, flood, blight, rot\n",
    "- Equipment: tractor, plow, harvester, sprayer, pump\n",
    "- Activities: planting, sowing, grazing, milking, breeding, aquaculture\n",
    "- Crops: rice, wheat, corn, tomato, potato, sugarcane, cotton, tea, coffee\n",
    "\n",
    "STRICT CLASSIFICATION RULES:\n",
    "1. If question contains ANY agricultural keyword â†’ AGRICULTURAL: YES\n",
    "2. Questions about farming methods/techniques â†’ AGRICULTURAL: YES\n",
    "3. Questions about growing/raising food â†’ AGRICULTURAL: YES\n",
    "4. Simple definitions like \"what is crop rotation\" â†’ AGRICULTURAL: YES\n",
    "5. ONLY mark NON-agricultural if clearly about: sports, movies, politics, technology, general knowledge with NO farming context\n",
    "\n",
    "GREETING DETECTION (mark YES only if):\n",
    "- Pure social: \"hi\", \"hello\", \"how are you\", \"thanks\", \"bye\"\n",
    "- Questions with agricultural words are NOT greetings\n",
    "\n",
    "HISTORY REFERENCE (mark YES only if):\n",
    "- Uses pronouns: \"it\", \"this\", \"that\", \"the same\"\n",
    "- Phrases: \"tell me more\", \"what about this\", \"continue\"\n",
    "\n",
    "CONVERSATION CONTEXT:\n",
    "{conversation_context}\n",
    "\n",
    "USER INPUT: \"{question}\"\n",
    "\n",
    "Respond in this EXACT format (no extra text):\n",
    "AGRICULTURAL: [YES/NO]\n",
    "GREETING: [YES/NO]\n",
    "REFERENCES_HISTORY: [YES/NO]\n",
    "INTERACTION_TYPE: [QUESTION/GREETING/THANKS/FOLLOWUP]\n",
    "REASONING: [One sentence explaining your decision]\n",
    "[/INST]\"\"\"\n",
    "\n",
    "        result = your_llm_pipeline(\n",
    "            intent_prompt,\n",
    "            max_new_tokens=80,\n",
    "            do_sample=False,\n",
    "            temperature=0.05,\n",
    "            pad_token_id=your_llm_pipeline.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        if result and len(result) > 0:\n",
    "            response = result[0]['generated_text'].strip()\n",
    "            \n",
    "            intent_analysis = {\n",
    "                'is_agricultural': 'AGRICULTURAL: YES' in response.upper(),\n",
    "                'is_greeting': 'GREETING: YES' in response.upper(),\n",
    "                'references_history': 'REFERENCES_HISTORY: YES' in response.upper(),\n",
    "                'interaction_type': 'question'\n",
    "            }\n",
    "            \n",
    "            for line in response.split('\\n'):\n",
    "                if line.startswith('INTERACTION_TYPE:'):\n",
    "                    intent_analysis['interaction_type'] = line.split(':', 1)[1].strip().lower()\n",
    "                    break\n",
    "            \n",
    "            print(f\"LLM Intent Analysis: {intent_analysis}\")\n",
    "            return intent_analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Intent analysis error: {e}\")\n",
    "    \n",
    "    return {\n",
    "        'is_agricultural': True,\n",
    "        'is_greeting': False,\n",
    "        'references_history': False,\n",
    "        'interaction_type': 'question'\n",
    "    }\n",
    "\n",
    "def generate_contextual_response_with_llm(intent_analysis, question, conversation_context, original_lang='en'):\n",
    "    \"\"\"Better greeting/redirect responses\"\"\"\n",
    "    global your_llm_pipeline\n",
    "    \n",
    "    if your_llm_pipeline is None:\n",
    "        return \"Hello! I'm your agricultural assistant. How can I help you today?\"\n",
    "    \n",
    "    try:\n",
    "        if intent_analysis['is_greeting'] or intent_analysis['interaction_type'] in ['greeting', 'thanks', 'goodbye']:\n",
    "            response_prompt = f\"\"\"<s>[INST] You are a friendly agricultural assistant. User sent a greeting/social message.\n",
    "\n",
    "USER MESSAGE: \"{question}\"\n",
    "TYPE: {intent_analysis['interaction_type']}\n",
    "\n",
    "Generate a SHORT, warm response (2-3 sentences) that:\n",
    "1. Responds naturally to their message\n",
    "2. Mentions you help with farming/agriculture  \n",
    "3. Invites a question\n",
    "\n",
    "Keep it brief and friendly. Don't list all topics.\n",
    "\n",
    "Response: [/INST]\"\"\"\n",
    "        else:\n",
    "            # Non-agricultural query redirect\n",
    "            response_prompt = f\"\"\"<s>[INST] You are an agricultural assistant. User asked a non-agricultural question.\n",
    "\n",
    "USER QUESTION: \"{question}\"\n",
    "\n",
    "Generate a polite response (2-3 sentences) that:\n",
    "1. Politely says you specialize in agriculture/farming\n",
    "2. Briefly mention what you can help with (crops, livestock, soil, etc.)\n",
    "3. Invite them to ask farming questions\n",
    "\n",
    "Keep it short and encouraging.\n",
    "\n",
    "Response: [/INST]\"\"\"\n",
    "        \n",
    "        result = your_llm_pipeline(\n",
    "            response_prompt,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.2,\n",
    "            do_sample=False,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=your_llm_pipeline.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        if result and len(result) > 0:\n",
    "            response = result[0]['generated_text'].strip()\n",
    "            \n",
    "            if original_lang != 'en':\n",
    "                response = translate_from_english(response, original_lang)\n",
    "            \n",
    "            return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Response generation error: {e}\")\n",
    "    \n",
    "    fallback = \"Hello! I'm your agricultural assistant for crops, livestock, soil, and farming. What would you like to know?\"\n",
    "    if original_lang != 'en':\n",
    "        fallback = translate_from_english(fallback, original_lang)\n",
    "    return fallback\n",
    "\n",
    "def rewrite_query_with_history(follow_up_question, history):\n",
    "    \"\"\"LLM-based query rewriting for conversational context\"\"\"\n",
    "    global your_llm_pipeline\n",
    "    \n",
    "    if not history or your_llm_pipeline is None:\n",
    "        return follow_up_question\n",
    "        \n",
    "    history_context = []\n",
    "    for exchange in history[-2:]:\n",
    "        user_input_en, _ = translate_to_english(exchange['user_input'])\n",
    "        bot_response_en, _ = translate_to_english(exchange['bot_response'])\n",
    "        \n",
    "        bot_response_summary = (bot_response_en[:200] + '...') if len(bot_response_en) > 200 else bot_response_en\n",
    "        \n",
    "        history_context.append(f\"User: {user_input_en}\")\n",
    "        history_context.append(f\"Assistant: {bot_response_summary}\")\n",
    "        \n",
    "    history_str = \"\\n\".join(history_context)\n",
    "\n",
    "    rewrite_prompt = f\"\"\"<s>[INST] Rewrite follow-up question into standalone question using conversation history.\n",
    "\n",
    "HISTORY:\n",
    "{history_str}\n",
    "\n",
    "FOLLOW-UP: \"{follow_up_question}\"\n",
    "\n",
    "Rewrite as clear standalone question. Example: \"tell me more about it\" â†’ \"Tell me more about tomato blight\"\n",
    "\n",
    "STANDALONE QUESTION: [/INST]\"\"\"\n",
    "\n",
    "    try:\n",
    "        result = your_llm_pipeline(\n",
    "            rewrite_prompt,\n",
    "            max_new_tokens=60,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            pad_token_id=your_llm_pipeline.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        if result and len(result) > 0:\n",
    "            rewritten_query = result[0]['generated_text'].strip().replace('\"', '')\n",
    "            \n",
    "            if len(rewritten_query) > len(follow_up_question) + 3 and len(rewritten_query) < 200:\n",
    "                print(f\"Rewritten query: '{rewritten_query}'\")\n",
    "                return rewritten_query\n",
    "        \n",
    "        return follow_up_question\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during query rewriting: {e}\")\n",
    "        return follow_up_question\n",
    "\n",
    "def generate_detailed_analysis(initial_answer, question, gemini_analysis=\"\", conversation_context=\"\", original_lang='en'):\n",
    "    \"\"\"ENHANCED detailed analysis - EXACTLY 400 WORDS with 800 tokens\"\"\"\n",
    "    global your_llm_pipeline\n",
    "    \n",
    "    if your_llm_pipeline is None:\n",
    "        return initial_answer + \"\\n\\n[Note: Detailed analysis unavailable]\"\n",
    "    \n",
    "    try:\n",
    "        # UPDATED PROMPT: Explicitly request 400 words\n",
    "        detailed_prompt = f\"\"\"<s>[INST] You are a senior agricultural consultant with 20+ years of field experience. Provide ONE comprehensive expert analysis in EXACTLY 400 WORDS.\n",
    "\n",
    "QUESTION: {question}\n",
    "BASIC ANSWER: {initial_answer}\n",
    "IMAGE ANALYSIS: {gemini_analysis}\n",
    "CONTEXT: {conversation_context}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. Write EXACTLY 400 words - not more, not less\n",
    "2. Write ONE unified expert response - NOT separate sections or bullet points\n",
    "3. Focus ONLY on what's most relevant to the specific question\n",
    "4. Keep it practical and actionable - avoid theoretical tangents\n",
    "5. Flow naturally from problem identification â†’ solution â†’ prevention\n",
    "6. Be specific with recommendations (exact products, dosages, timing)\n",
    "7. Write in cohesive paragraph style\n",
    "\n",
    "STRUCTURE YOUR RESPONSE AS A FLOWING NARRATIVE:\n",
    "- Start with the core issue/topic and its immediate significance\n",
    "- Explain the underlying cause or mechanism briefly\n",
    "- Provide specific, actionable solution steps with exact details\n",
    "- Include timing, dosage, or frequency where applicable\n",
    "- Mention 2-3 preventive measures naturally in the flow\n",
    "- End with one practical monitoring/follow-up tip\n",
    "\n",
    "AVOID:\n",
    "- Don't create separate numbered sections or headers\n",
    "- Don't jump between multiple unrelated aspects\n",
    "- Don't give generic advice - be specific to THIS question\n",
    "- Don't use bullet points - write in paragraph form\n",
    "- Don't repeat information from the basic answer unnecessarily\n",
    "\n",
    "Write a SINGLE cohesive expert consultation response in EXACTLY 400 WORDS:\n",
    "\n",
    "[/INST]\"\"\"\n",
    "\n",
    "        detailed_result = your_llm_pipeline(\n",
    "            detailed_prompt,\n",
    "            max_new_tokens=800,  # UPDATED: 800 tokens for ~400 words\n",
    "            temperature=0.2,\n",
    "            do_sample=False,\n",
    "            top_p=0.85,\n",
    "            return_full_text=False\n",
    "        )\n",
    "        \n",
    "        if detailed_result and len(detailed_result) > 0:\n",
    "            detailed_analysis = detailed_result[0]['generated_text'].strip()\n",
    "            \n",
    "            # Clean up any remaining headers or bullets that might appear\n",
    "            detailed_analysis = re.sub(r'^\\d+\\.\\s+\\*\\*.*?\\*\\*:?\\s*', '', detailed_analysis, flags=re.MULTILINE)\n",
    "            detailed_analysis = re.sub(r'^\\*\\*.*?\\*\\*:?\\s*', '', detailed_analysis, flags=re.MULTILINE)\n",
    "            detailed_analysis = re.sub(r'^[-â€¢]\\s+', '', detailed_analysis, flags=re.MULTILINE)\n",
    "            \n",
    "            if len(detailed_analysis) > 200:\n",
    "                enhanced_response = detailed_analysis\n",
    "                \n",
    "                if original_lang != 'en':\n",
    "                    try:\n",
    "                        enhanced_response = translate_from_english(enhanced_response, original_lang)\n",
    "                    except Exception:\n",
    "                        enhanced_response += f\"\\n\\n[Note: Analysis provided in English due to translation limitations]\"\n",
    "                \n",
    "                return enhanced_response\n",
    "        \n",
    "        return initial_answer \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in detailed analysis: {e}\")\n",
    "        return initial_answer + f\"\\n\\n**Note:** Enhanced analysis encountered an error.\"\n",
    "\n",
    "def initialize_speech_recognition():\n",
    "    global recognizer\n",
    "    try:\n",
    "        recognizer = sr.Recognizer()\n",
    "        recognizer.energy_threshold = 4000\n",
    "        recognizer.dynamic_energy_threshold = True\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def initialize_gemini_model():\n",
    "    global gemini_model\n",
    "    try:\n",
    "        genai.configure(api_key=GEMINI_API_KEY)\n",
    "        test_model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "        test_response = test_model.generate_content(\"Test\")\n",
    "        if test_response and test_response.text:\n",
    "            gemini_model = test_model\n",
    "            return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini init error: {e}\")\n",
    "        return False\n",
    "\n",
    "def analyze_agricultural_image_with_gemini(image):\n",
    "    global gemini_model\n",
    "    if image is None or gemini_model is None:\n",
    "        return \"\", \"\", False\n",
    "    \n",
    "    try:\n",
    "        if not isinstance(image, Image.Image):\n",
    "            image = Image.fromarray(image).convert('RGB')\n",
    "        else:\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        max_size = (1024, 1024)\n",
    "        if image.size[0] > max_size[0] or image.size[1] > max_size[1]:\n",
    "            image.thumbnail(max_size, Image.Resampling.LANCZOS)\n",
    "        \n",
    "        prompt = \"\"\"Analyze this agricultural image precisely.\n",
    "\n",
    "AGRICULTURAL: [YES/NO]\n",
    "CATEGORY: [CROP/SOIL/LIVESTOCK/PEST/EQUIPMENT/DISEASE]\n",
    "IDENTIFICATION: [Specific name]\n",
    "CONDITION: [Specific problem or \"Healthy\"]\n",
    "CONFIDENCE: [HIGH/MEDIUM/LOW]\n",
    "\n",
    "If NOT agricultural, respond:\n",
    "AGRICULTURAL: NO\n",
    "REASON: [Why not]\"\"\"\n",
    "\n",
    "        response = gemini_model.generate_content(\n",
    "            [prompt, image],\n",
    "            generation_config=genai.types.GenerationConfig(temperature=0.1, max_output_tokens=800)\n",
    "        )\n",
    "        \n",
    "        if not response or not response.text:\n",
    "            return \"Analysis failed\", \"Unknown\", False\n",
    "        \n",
    "        analysis_text = response.text.strip()\n",
    "        is_agricultural = 'AGRICULTURAL: YES' in analysis_text.upper()\n",
    "        \n",
    "        identification = \"\"\n",
    "        for line in analysis_text.split('\\n'):\n",
    "            if line.startswith('IDENTIFICATION:'):\n",
    "                identification = line.split(':', 1)[1].strip() if ':' in line else \"\"\n",
    "        \n",
    "        return analysis_text, identification or \"Agricultural subject\", is_agricultural\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", \"Error\", False\n",
    "\n",
    "def load_vectorstore(index_path):\n",
    "    try:\n",
    "        embedder = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs={'device': 'cuda:0' if torch.cuda.is_available() else 'cpu'}\n",
    "        )\n",
    "        vectorstore = FAISS.load_local(index_path, embedder, allow_dangerous_deserialization=True)\n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        print(f\"Vectorstore load error: {e}\")\n",
    "        return None\n",
    "\n",
    "def build_qa_chain(vectorstore):\n",
    "    global your_llm_pipeline\n",
    "    try:\n",
    "        model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        device_map = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, torch_dtype=torch_dtype, device_map=device_map,\n",
    "            trust_remote_code=True, low_cpu_mem_usage=True, use_cache=True\n",
    "        ).eval()\n",
    "        \n",
    "        pipe = pipeline(\n",
    "            \"text-generation\", model=model, tokenizer=tokenizer,\n",
    "            max_new_tokens=256, temperature=0.1, do_sample=False,\n",
    "            return_full_text=False, pad_token_id=tokenizer.eos_token_id, batch_size=1\n",
    "        )\n",
    "        \n",
    "        your_llm_pipeline = pipe\n",
    "        llm = HuggingFacePipeline(pipeline=pipe)\n",
    "        retriever = vectorstore.as_retriever(search_type=\"similarity\", k=2)\n",
    "        return RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "    except Exception as e:\n",
    "        print(f\"QA chain error: {e}\")\n",
    "        return None\n",
    "\n",
    "# ===== MAIN ANSWER FUNCTION WITH DYNAMIC DATA & ENHANCED LOCATION =====\n",
    "\n",
    "def answer_question_core(question, image=None, use_detailed_analysis=False, actual_session_id=None):\n",
    "    global qa_chain\n",
    "    \n",
    "    if qa_chain is None:\n",
    "        return \"System not initialized\"\n",
    "    \n",
    "    if not question.strip() and image is None:\n",
    "        return \"Please enter a question or upload an image\"\n",
    "    \n",
    "    try:\n",
    "        original_question = question\n",
    "        original_lang = 'en'\n",
    "        \n",
    "        # Language detection and translation\n",
    "        if question.strip():\n",
    "            original_lang = detect_language(question.strip())\n",
    "            if original_lang != 'en':\n",
    "                english_question, _ = translate_to_english(question.strip())\n",
    "                question = english_question\n",
    "        \n",
    "        # Get conversation context for intent analysis\n",
    "        conversation_context = get_conversation_context(actual_session_id)\n",
    "        \n",
    "        # ===== STEP 1: ANALYZE USER INTENT WITH LLM (from version1) =====\n",
    "        intent_analysis = analyze_user_intent_with_llm(question, conversation_context)\n",
    "        \n",
    "        # Handle greetings\n",
    "        if intent_analysis['is_greeting'] or intent_analysis['interaction_type'] in ['greeting', 'thanks', 'goodbye']:\n",
    "            response = generate_contextual_response_with_llm(intent_analysis, question, conversation_context, original_lang)\n",
    "            session_manager.add_to_history(actual_session_id, original_question, response, \"\", original_lang, use_detailed_analysis, intent_analysis['interaction_type'])\n",
    "            return response\n",
    "        \n",
    "        # ===== STEP 2: DETECT QUESTION INTENT DYNAMICALLY =====\n",
    "        question_intent = detect_question_intent_with_gemini(question)\n",
    "        \n",
    "        # ===== STEP 2: HANDLE LOCATION SETTING =====\n",
    "        if question_intent == \"LOCATION_SETTING\":\n",
    "            location_setting_patterns = [\n",
    "                r'i am (?:in|at|from|near)\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)',\n",
    "                r'my (?:location|place|city|area) is\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)',\n",
    "                r'(?:set|save|remember) (?:my )?location (?:as|to)?\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)'\n",
    "            ]\n",
    "            \n",
    "            for pattern in location_setting_patterns:\n",
    "                match = re.search(pattern, question, re.IGNORECASE)\n",
    "                if match:\n",
    "                    new_location = match.group(1)\n",
    "                    # Geocode and save location\n",
    "                    geocoded = geocode_location(new_location)\n",
    "                    if geocoded.get('success'):\n",
    "                        session_manager.set_location(actual_session_id, geocoded)\n",
    "                        response = f\"\"\"âœ… **Location Saved Successfully!**\n",
    "\n",
    "ðŸ“ **Location:** {geocoded['name']}\n",
    "ðŸŒ **Coordinates:** {geocoded['latitude']:.4f}Â°N, {geocoded['longitude']:.4f}Â°E\n",
    "\n",
    "Now you can ask questions using \"here\", \"my location\", or \"my area\"!\n",
    "\n",
    "**Try these:**\n",
    "- \"What is the best crop here?\"\n",
    "- \"Show me weather forecast for my area\"\n",
    "- \"Which crops suit my location?\"\n",
    "- \"What's the soil like here?\"\n",
    "\n",
    "Your location is saved for this session only.\"\"\"\n",
    "                        \n",
    "                        if original_lang != 'en':\n",
    "                            response = translate_from_english(response, original_lang)\n",
    "                        \n",
    "                        session_manager.add_to_history(\n",
    "                            actual_session_id, original_question, response, \"\", \n",
    "                            original_lang, use_detailed_analysis, \"location_setting\",\n",
    "                            new_location, None\n",
    "                        )\n",
    "                        \n",
    "                        return response\n",
    "                    else:\n",
    "                        return f\"âŒ Could not find location: {new_location}. Please check spelling or try a nearby city.\"\n",
    "        \n",
    "        # ===== STEP 3: DETECT LOCATION IN QUESTION =====\n",
    "        location_query = None\n",
    "        \n",
    "        # First try explicit location patterns (case-insensitive, limit to 1-3 words)\n",
    "        location_patterns = [\n",
    "            r'in\\s+([A-Za-z]+(?:\\s+[A-Za-z]+){0,2})(?:\\s|,|\\?|$)',\n",
    "            r'at\\s+([A-Za-z]+(?:\\s+[A-Za-z]+){0,2})(?:\\s|,|\\?|$)',\n",
    "            r'for\\s+([A-Za-z]+(?:\\s+[A-Za-z]+){0,2})(?:\\s|,|\\?|$)',\n",
    "            r'near\\s+([A-Za-z]+(?:\\s+[A-Za-z]+){0,2})(?:\\s|,|\\?|$)',\n",
    "            r'around\\s+([A-Za-z]+(?:\\s+[A-Za-z]+){0,2})(?:\\s|,|\\?|$)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in location_patterns:\n",
    "            match = re.search(pattern, question, re.IGNORECASE)\n",
    "            if match:\n",
    "                potential_location = match.group(1).strip()\n",
    "                # Filter out common non-location words\n",
    "                non_location_words = ['here', 'there', 'my', 'this', 'that', 'the', 'a', 'an', \n",
    "                                     'which', 'what', 'where', 'when', 'how', 'best', 'good',\n",
    "                                     'suitable', 'crops', 'crop', 'farming', 'agriculture',\n",
    "                                     'growing', 'cultivation', 'season', 'weather', 'soil']\n",
    "                if potential_location.lower() not in non_location_words:\n",
    "                    location_query = potential_location\n",
    "                    print(f\"[Location] Explicit location found: {location_query}\")\n",
    "                    break\n",
    "        \n",
    "        # ===== STEP 4: HANDLE CONTEXT-DEPENDENT LOCATION WORDS =====\n",
    "        if not location_query:\n",
    "            context_location_words = ['here', 'my location', 'my area', 'nearby', \n",
    "                                     'this place', 'current location', 'my region',\n",
    "                                     'this area', 'this region', 'local']\n",
    "            \n",
    "            question_lower = question.lower()\n",
    "            if any(word in question_lower for word in context_location_words):\n",
    "                # Try to retrieve location from session cache\n",
    "                cached_location = session_manager.get_location(actual_session_id)\n",
    "                if cached_location and isinstance(cached_location, dict):\n",
    "                    location_query = cached_location.get('name', None)\n",
    "                    print(f\"[Session {actual_session_id[-8:]}] Using cached location: {location_query}\")\n",
    "                else:\n",
    "                    # No cached location - ask user to specify\n",
    "                    prompt_response = \"\"\"ðŸ” **Location Required**\n",
    "\n",
    "I understand you're asking about crops for your location (\"here\"), but I need to know your specific location to provide accurate recommendations with **real-time weather and soil data**.\n",
    "\n",
    "**Please do ONE of the following:**\n",
    "\n",
    "**Option 1:** Specify location in your question\n",
    "- Example: \"What is the best crop in Pune?\"\n",
    "- Example: \"Best crops for Mumbai, Maharashtra\"\n",
    "\n",
    "**Option 2:** Set your location first\n",
    "- Say: \"I am in [Your City/District]\"\n",
    "- Example: \"I am in Bangalore\"\n",
    "\n",
    "**Option 3:** Ask a general question\n",
    "- I'll provide general guidelines without location-specific data\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ’¡ **Why location matters:**\n",
    "âœ… Real-time 15-day weather forecast\n",
    "âœ… AI-powered soil analysis for your area\n",
    "âœ… Crop suitability scores (0-100)\n",
    "âœ… Location-specific recommendations\n",
    "\n",
    "Once you set your location, you can use \"here\" in all future questions!\"\"\"\n",
    "                    \n",
    "                    if original_lang != 'en':\n",
    "                        prompt_response = translate_from_english(prompt_response, original_lang)\n",
    "                    \n",
    "                    return prompt_response\n",
    "        \n",
    "        # ===== STEP 5: DYNAMIC CROP RECOMMENDATION PATH =====\n",
    "        if question_intent == \"CROP_RECOMMENDATION\" and location_query:\n",
    "            print(f\"[Dynamic] Fetching live data for: {location_query}\")\n",
    "            crop_recommendation = recommend_best_crops(location_query, actual_session_id)\n",
    "            \n",
    "            if crop_recommendation.get('success'):\n",
    "                dynamic_data = crop_recommendation\n",
    "                \n",
    "                # Format response with live data\n",
    "                weather = crop_recommendation['weather_summary']\n",
    "                soil = crop_recommendation['soil_summary']\n",
    "                top_crops = crop_recommendation['top_crops'][:3]\n",
    "                forecast_days = weather.get('forecast_days', 15)\n",
    "              \n",
    "                # Brief Weather Summary\n",
    "                response = f\"\"\"**Weather (Next {forecast_days} Days):** {weather['avg_temp']:.1f}Â°C avg, {weather['total_rainfall_15d']:.1f}mm rain, {weather['avg_humidity']:.0f}% humidity\\n\\n\"\"\"\n",
    "                \n",
    "                # Brief Soil Summary\n",
    "                response += f\"\"\"**Soil:** {soil['soil_type'].replace('_', ' ').title()}, pH {soil['ph']:.1f}, {soil.get('fertility', 'Medium').title()} fertility\\n\\n\"\"\"\n",
    "                \n",
    "                response += f\"\"\"**TOP 3 RECOMMENDED CROPS:**\\n\\n\"\"\"\n",
    "                \n",
    "                for i, crop_data in enumerate(top_crops, 1):\n",
    "                    crop_name = crop_data['crop'].title()\n",
    "                    score = crop_data['score']\n",
    "                    suitable = \"HIGHLY SUITABLE\" if score >= 80 else \"MODERATELY SUITABLE\" if score >= 60 else \"NOT RECOMMENDED\"\n",
    "                    \n",
    "                    response += f\"\"\"**{i}. {crop_name}** - Score: {score}/100 {suitable}\\n\"\"\"\n",
    "                    \n",
    "                    req = crop_data['requirements']\n",
    "                    response += f\"\"\"   â€¢ Duration: {req.get('duration_days', 'N/A')} days | Water: {req.get('water_requirement', 'N/A').title()}\\n\"\"\"\n",
    "                    \n",
    "                    # Fetch and add price prediction\n",
    "                    price_data = fetch_crop_prices(crop_data['crop'], location_query)\n",
    "                    if price_data.get('success'):\n",
    "                        current_avg = price_data['current_price_per_quintal']['average']\n",
    "                        trend = price_data['market_trend']\n",
    "                        pred_avg = (price_data['price_prediction_3months']['expected_min'] + price_data['price_prediction_3months']['expected_max']) / 2\n",
    "                        trend_icon = \"ðŸ“ˆ\" if trend == \"rising\" else \"ðŸ“‰\" if trend == \"falling\" else \"âž¡ï¸\"\n",
    "                        response += f\"\"\"   â€¢ Current Price: â‚¹{current_avg:.0f}/quintal {trend_icon} | 3-Month Forecast: â‚¹{pred_avg:.0f}/quintal\\n\"\"\"\n",
    "                    \n",
    "                    if crop_data['issues']:\n",
    "                        response += f\"\"\" Note: {crop_data['issues'][0]}\\n\"\"\"\n",
    "                    \n",
    "                    response += \"\\n\"\n",
    "                \n",
    "                # Generate follow-up questions\n",
    "                followup_questions = generate_contextual_followup_questions(\n",
    "                    question, \n",
    "                    response,\n",
    "                    {\n",
    "                        'location': location_query,\n",
    "                        'has_weather': True,\n",
    "                        'top_crop': top_crops[0]['crop'] if top_crops else None\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                if followup_questions:\n",
    "                    response += \"\\n\\n**ðŸ’¡Similar questions:**\\n\"\n",
    "                    for i, fq in enumerate(followup_questions, 1):\n",
    "                        response += f\"{i}. {fq}\\n\"\n",
    "                \n",
    "                # Translate if needed\n",
    "                if original_lang != 'en':\n",
    "                    try:\n",
    "                        response = translate_from_english(response, original_lang)\n",
    "                    except:\n",
    "                        response += \"\\n\\n[Note: Response in English due to translation limits]\"\n",
    "                \n",
    "                # Save to history\n",
    "                session_manager.add_to_history(\n",
    "                    actual_session_id, original_question, response, \"\", \n",
    "                    original_lang, use_detailed_analysis, \"crop_recommendation\",\n",
    "                    location_query, dynamic_data\n",
    "                )\n",
    "                \n",
    "                return response\n",
    "            else:\n",
    "                # Failed to get crop recommendation data\n",
    "                error_msg = crop_recommendation.get('error', 'Unknown error')\n",
    "                return f\"âŒ Unable to fetch live data: {error_msg}\\n\\nPlease try again or ask a general question.\"\n",
    "        \n",
    "        # ===== STEP 6: IMAGE ANALYSIS =====\n",
    "        gemini_full_analysis = \"\"\n",
    "        gemini_summary = \"\"\n",
    "        image_is_agricultural = False\n",
    "        \n",
    "        if image is not None:\n",
    "            print(\"Analyzing image with Gemini...\")\n",
    "            gemini_full_analysis, gemini_summary, image_is_agricultural = analyze_agricultural_image_with_gemini(image)\n",
    "            \n",
    "            if not image_is_agricultural:\n",
    "                response = f\"\"\"Non-Agricultural Image Detected\n",
    "\n",
    "The uploaded image doesn't appear to be related to agriculture, farming, livestock, dairy, fisheries, horticulture, or other agricultural topics.\n",
    "\n",
    "Analysis: {gemini_full_analysis}\n",
    "\n",
    "I specialize in agricultural topics including:\n",
    "- Crops and plant cultivation\n",
    "- Livestock and animal husbandry  \n",
    "- Dairy farming and milk production\n",
    "- Fisheries and aquaculture\n",
    "- Horticulture and gardening\n",
    "- Forestry and tree cultivation\n",
    "- Soil management and fertilization\n",
    "- Agricultural equipment and machinery\n",
    "- Plant and animal diseases\n",
    "- Pest management\n",
    "\n",
    "Please upload an agriculture-related image!\"\"\"\n",
    "                \n",
    "                if original_lang != 'en':\n",
    "                    response = translate_from_english(response, original_lang)\n",
    "                \n",
    "                session_manager.add_to_history(\n",
    "                    actual_session_id, original_question, response, \n",
    "                    \"Non-agricultural image\", original_lang, use_detailed_analysis\n",
    "                )\n",
    "                return response\n",
    "        \n",
    "        # ===== REFINED: Better non-agricultural detection (from version1) =====\n",
    "        if not intent_analysis['is_agricultural'] and not image_is_agricultural:\n",
    "            response = generate_contextual_response_with_llm(intent_analysis, question, conversation_context, original_lang)\n",
    "            session_manager.add_to_history(actual_session_id, original_question, response, \"\", original_lang, use_detailed_analysis)\n",
    "            return response\n",
    "        \n",
    "        # ===== QUERY REWRITING WITH HISTORY (from version1) =====\n",
    "        conversation_history = session_manager.get_history(actual_session_id)\n",
    "        if intent_analysis['references_history']:\n",
    "            rewritten_question = rewrite_query_with_history(question, conversation_history)\n",
    "            if rewritten_question.lower() != question.lower():\n",
    "                print(f\"[History Rewrite] Original: '{question}' â†’ Rewritten: '{rewritten_question}'\")\n",
    "                question = rewritten_question\n",
    "        \n",
    "        # ===== STEP 7: STANDARD RAG QUERY WITH CONVERSATION CONTEXT =====\n",
    "        rag_query_parts = []\n",
    "        \n",
    "        # Add current question\n",
    "        if question.strip():\n",
    "            rag_query_parts.append(f\"Current question: {question.strip()}\")\n",
    "        elif image is not None:\n",
    "            rag_query_parts.append(f\"Analyzing uploaded image showing: {gemini_summary}\")\n",
    "        \n",
    "        # Add image analysis if available\n",
    "        if gemini_summary:\n",
    "            rag_query_parts.append(f\"Image analysis shows: {gemini_summary}\")\n",
    "        \n",
    "        # Add conversation context for better RAG retrieval\n",
    "        if conversation_context:\n",
    "            rag_query_parts.append(conversation_context)\n",
    "        \n",
    "        comprehensive_query = \"\\n\\n\".join(rag_query_parts)\n",
    "        \n",
    "        print(f\"[Session {actual_session_id[-8:]}] Querying RAG with intent: {question_intent}\")\n",
    "        rag_result = qa_chain.run(comprehensive_query)\n",
    "        rag_result = clean_answer(rag_result)\n",
    "        \n",
    "        final_response = rag_result\n",
    "        \n",
    "        # ===== APPLY EXPERT ANALYSIS IF REQUESTED (from version1) =====\n",
    "        if use_detailed_analysis:\n",
    "            print(f\"[Session {actual_session_id[-8:]}] Applying detailed analysis...\")\n",
    "            final_response = generate_detailed_analysis(\n",
    "                rag_result, \n",
    "                question, \n",
    "                gemini_summary, \n",
    "                conversation_context, \n",
    "                'en'\n",
    "            )\n",
    "        \n",
    "        cleanup_memory()\n",
    "        \n",
    "        # Add image analysis if available (from version1 format)\n",
    "        if gemini_full_analysis and image_is_agricultural:\n",
    "            lines = gemini_full_analysis.split('\\n')\n",
    "            category = \"\"\n",
    "            identification = \"\"\n",
    "            condition = \"\"\n",
    "            \n",
    "            for line in lines:\n",
    "                if line.startswith('CATEGORY:'):\n",
    "                    category = line.split(':', 1)[1].strip()\n",
    "                elif line.startswith('IDENTIFICATION:'):\n",
    "                    identification = line.split(':', 1)[1].strip()\n",
    "                elif line.startswith('CONDITION:'):\n",
    "                    condition = line.split(':', 1)[1].strip()\n",
    "            \n",
    "            detection_header = f\"**{category.title()} Detected:**\" if use_detailed_analysis else f\"**{category.title()}:**\"\n",
    "            \n",
    "            response = f\"{detection_header} {identification}\"\n",
    "            if condition and condition.lower() not in [\"healthy\", \"normal\", \"none\"]:\n",
    "                response += f\" - {condition}\"\n",
    "            \n",
    "            response += f\"\\n\\n{final_response}\"\n",
    "            final_response = response\n",
    "        \n",
    "        # Generate contextual follow-ups\n",
    "        followup_questions = generate_contextual_followup_questions(\n",
    "            question, \n",
    "            final_response,\n",
    "            {\n",
    "                'location': session_manager.get_location(actual_session_id),\n",
    "                'has_weather': False,\n",
    "                'has_image': image is not None\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if followup_questions:\n",
    "            final_response += \"\\n\\n**ðŸ’¡Similar questions:**\\n\"\n",
    "            for i, fq in enumerate(followup_questions, 1):\n",
    "                final_response += f\"{i}. {fq}\\n\"\n",
    "        \n",
    "        cleanup_memory()\n",
    "        \n",
    "        if original_lang != 'en':\n",
    "            try:\n",
    "                final_response = translate_from_english(final_response, original_lang)\n",
    "            except:\n",
    "                final_response += \"\\n\\n[Note: Response in English]\"\n",
    "        \n",
    "        # Determine question type for history tracking\n",
    "        question_type = intent_analysis.get('interaction_type', 'question')\n",
    "        if intent_analysis['references_history']:\n",
    "            question_type = 'followup'\n",
    "        \n",
    "        session_manager.add_to_history(\n",
    "            actual_session_id, original_question, final_response, \n",
    "            gemini_summary, original_lang, use_detailed_analysis, \n",
    "            question_type, location_query, None\n",
    "        )\n",
    "        \n",
    "        return final_response\n",
    "            \n",
    "    except Exception as e:\n",
    "        cleanup_memory()\n",
    "        return f\"âŒ Error: {str(e)}\"\n",
    "\n",
    "def answer_question(question, image=None, use_detailed_analysis=False, provided_session_id=None):\n",
    "    \"\"\"CONCURRENT-SAFE answer function\"\"\"\n",
    "    final_session_id = provided_session_id or \"default_session\"\n",
    "    actual_session_id = session_manager.get_session_id(request=None, provided_session_id=final_session_id)\n",
    "    \n",
    "    print(f\"[Session {actual_session_id[-8:]}] Request received\")\n",
    "    session_manager.increment_active_requests(actual_session_id)\n",
    "    \n",
    "    try:\n",
    "        with request_semaphore:\n",
    "            print(f\"[Session {actual_session_id[-8:]}] Processing...\")\n",
    "            result = answer_question_core(question, image, use_detailed_analysis, actual_session_id)\n",
    "            print(f\"[Session {actual_session_id[-8:]}] Completed\")\n",
    "            return result\n",
    "    finally:\n",
    "        session_manager.decrement_active_requests(actual_session_id)\n",
    "\n",
    "# ===== API FUNCTIONS =====\n",
    "\n",
    "def fetch_session_id_api(request: gr.Request = None):\n",
    "    return session_manager.fetch_session_id(request)\n",
    "\n",
    "def answer_question_with_session(question, image=None, use_detailed_analysis=False, session_id=None):\n",
    "    return answer_question(question, image, use_detailed_analysis, session_id)\n",
    "\n",
    "def get_conversation_history_by_session(session_id):\n",
    "    if not session_id:\n",
    "        return {\"status\": \"error\", \"message\": \"Session ID required\"}\n",
    "    try:\n",
    "        actual_session_id = session_manager.get_session_id(request=None, provided_session_id=session_id)\n",
    "        history = get_conversation_history_display(actual_session_id)\n",
    "        return {\"status\": \"success\", \"history\": history, \"session_id\": session_id}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "def clear_conversation_history_by_session(session_id):\n",
    "    if not session_id:\n",
    "        return {\"status\": \"error\", \"message\": \"Session ID required\"}\n",
    "    try:\n",
    "        actual_session_id = session_manager.get_session_id(request=None, provided_session_id=session_id)\n",
    "        session_manager.clear_session_history(actual_session_id)\n",
    "        return {\"status\": \"success\", \"message\": f\"History cleared for session: {session_id}\"}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "def get_similar_questions_api(user_question, location=None, topic=None):\n",
    "    \"\"\"API endpoint for getting similar questions - SEPARATE ENDPOINT\"\"\"\n",
    "    try:\n",
    "        context_data = {}\n",
    "        if location:\n",
    "            context_data['location'] = location\n",
    "        if topic:\n",
    "            context_data['topic'] = topic\n",
    "        \n",
    "        result = get_similar_questions_for_query(user_question, context_data if context_data else None)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "def initialize_system():\n",
    "    global qa_chain\n",
    "    try:\n",
    "        cleanup_memory()\n",
    "        \n",
    "        print(\"Initializing speech recognition...\")\n",
    "        initialize_speech_recognition()\n",
    "        \n",
    "        print(\"Initializing Gemini model...\")\n",
    "        gemini_success = initialize_gemini_model()\n",
    "        if not gemini_success:\n",
    "            return \"Error: Gemini API initialization failed\"\n",
    "        \n",
    "        if not os.path.exists(INDEX_PATH):\n",
    "            return \"Error: FAISS index not found\"\n",
    "            \n",
    "        print(\"Loading FAISS index...\")\n",
    "        vectordb = load_vectorstore(INDEX_PATH)\n",
    "        if vectordb is None:\n",
    "            return \"Error: Failed to load FAISS index\"\n",
    "        \n",
    "        print(\"Loading Mistral model...\")\n",
    "        qa_chain = build_qa_chain(vectordb)\n",
    "        if qa_chain is None:\n",
    "            return \"Error: Failed to build QA chain\"\n",
    "        \n",
    "        cleanup_memory()\n",
    "        \n",
    "        supported_langs = \", \".join([f\"{name}\" for name in SUPPORTED_LANGUAGES.values()])\n",
    "        \n",
    "        return f\"\"\"âœ… ENHANCED REAL-TIME DYNAMIC AgroRAG System Ready!\n",
    "\n",
    "ðŸš€ **ALL FEATURES FROM VERSION1 + VERSION2:**\n",
    "âœ“ LLM-driven agricultural intelligence (version1)\n",
    "âœ“ Expert analysis mode (EXACTLY 400 words, 800 tokens) (version1)\n",
    "âœ“ Non-agricultural question restriction (version1)\n",
    "âœ“ Query rewriting with conversation history (version1)\n",
    "âœ“ Stricter agricultural image classification (version1)\n",
    "âœ“ Intent analysis with LLM (version1)\n",
    "âœ“ Greeting/thanks handling (version1)\n",
    "âœ“ Dynamic Intent Detection (Gemini AI) (version2)\n",
    "âœ“ \"Here\" location support with session memory (version2)\n",
    "âœ“ Location setting: \"I am in [City]\" (version2)\n",
    "âœ“ Concurrent processing ({MAX_CONCURRENT_REQUESTS} parallel)\n",
    "âœ“ Live weather data - 15 days (Open-Meteo API) (version2)\n",
    "âœ“ AI-powered soil analysis (Gemini AI) (version2)\n",
    "âœ“ Dynamic crop requirements (Gemini AI) (version2)\n",
    "âœ“ Crop price prediction with 3-month forecast (Gemini AI) (version2)\n",
    "âœ“ Contextual follow-up suggestions (version2)\n",
    "âœ“ Similar questions API endpoint (version2)\n",
    "âœ“ Location-based recommendations (version2)\n",
    "âœ“ Multilingual: {supported_langs}\n",
    "\n",
    "**Try asking:**\n",
    "- \"I am in Pune\" (sets location)\n",
    "- \"What is the best crop here?\" (uses saved location)\n",
    "- \"Best crops for Mumbai\" (gets weather, soil, prices)\n",
    "- \"Tell me more about it\" (rewrites with history)\n",
    "- \"Hello\" (gets friendly greeting)\n",
    "\n",
    "**API Endpoints:**\n",
    "- get_similar_questions_api(question, location, topic)\n",
    "- fetch_crop_prices(crop_name, location)\n",
    "\"\"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error initializing system: {str(e)}\"\n",
    "\n",
    "print(\"Initializing ENHANCED REAL-TIME DYNAMIC AgroRAG System...\")\n",
    "init_status = initialize_system()\n",
    "print(init_status)\n",
    "\n",
    "# ===== UTILITY FUNCTIONS =====\n",
    "\n",
    "def transcribe_audio(audio):\n",
    "    if audio is None or recognizer is None:\n",
    "        return \"Audio unavailable\"\n",
    "    try:\n",
    "        sample_rate, audio_data = audio\n",
    "        if audio_data.dtype == np.int16:\n",
    "            audio_data = audio_data.astype(np.float32) / 32768.0\n",
    "        audio_data = (audio_data * 32767).astype(np.int16)\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_file:\n",
    "            with wave.open(tmp_file.name, 'wb') as wav_file:\n",
    "                wav_file.setnchannels(1 if len(audio_data.shape) == 1 else audio_data.shape[1])\n",
    "                wav_file.setsampwidth(2)\n",
    "                wav_file.setframerate(sample_rate)\n",
    "                wav_file.writeframes(audio_data.tobytes())\n",
    "            \n",
    "            with sr.AudioFile(tmp_file.name) as source:\n",
    "                recognizer.adjust_for_ambient_noise(source, duration=0.5)\n",
    "                audio = recognizer.record(source)\n",
    "                try:\n",
    "                    return recognizer.recognize_google(audio)\n",
    "                except:\n",
    "                    return \"Could not understand audio\"\n",
    "        \n",
    "        try:\n",
    "            os.unlink(tmp_file.name)\n",
    "        except:\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def transcribe_only(audio):\n",
    "    if audio is not None:\n",
    "        return transcribe_audio(audio)\n",
    "    return \"\"\n",
    "\n",
    "def get_combined_input(transcription, text_input_val):\n",
    "    if transcription and transcription.strip():\n",
    "        return transcription.strip()\n",
    "    elif text_input_val and text_input_val.strip():\n",
    "        return text_input_val.strip()\n",
    "    return \"\"\n",
    "\n",
    "def process_basic_question(transcription, text_input_val, image, session_id, request: gr.Request = None):\n",
    "    if session_id and session_id.strip():\n",
    "        final_session_id = session_id.strip()\n",
    "    else:\n",
    "        final_session_id = session_manager.get_session_id(request)\n",
    "    \n",
    "    question = get_combined_input(transcription, text_input_val)\n",
    "    if not question and image is None:\n",
    "        return \"Please provide a question or upload an image.\"\n",
    "    \n",
    "    return answer_question(question, image, use_detailed_analysis=False, provided_session_id=final_session_id)\n",
    "\n",
    "def process_expert_question(transcription, text_input_val, image, session_id, request: gr.Request = None):\n",
    "    \"\"\"Process question with expert analysis (Web UI) - CONCURRENT SAFE\"\"\"\n",
    "    if session_id and session_id.strip():\n",
    "        final_session_id = session_id.strip()\n",
    "    else:\n",
    "        final_session_id = session_manager.get_session_id(request)\n",
    "    \n",
    "    question = get_combined_input(transcription, text_input_val)\n",
    "    if not question and image is None:\n",
    "        return \"Please provide a question or upload an image.\"\n",
    "    \n",
    "    result = answer_question(question, image, use_detailed_analysis=True, provided_session_id=final_session_id)\n",
    "    return f\"**EXPERT ANALYSIS (400 words)** \\n\\n{result}\"\n",
    "\n",
    "def clear_all_inputs():\n",
    "    return \"\", \"\", None, \"\"\n",
    "\n",
    "def refresh_history(request: gr.Request):\n",
    "    session_id = session_manager.get_session_id(request)\n",
    "    return get_conversation_history_display(session_id)\n",
    "\n",
    "def clear_conversation_history(request: gr.Request):\n",
    "    session_id = session_manager.get_session_id(request)\n",
    "    session_manager.clear_session_history(session_id)\n",
    "    return f\"âœ… Your conversation history has been cleared!\\n\\nSession ID: {session_id}\"\n",
    "\n",
    "def update_system_status(request: gr.Request = None):\n",
    "    stats = session_manager.get_session_stats()\n",
    "    current_session_id = session_manager.get_session_id(request) if request else \"Unknown\"\n",
    "    \n",
    "    return f\"\"\"âœ… ENHANCED REAL-TIME DYNAMIC AgroRAG System: Active\n",
    "\n",
    "ðŸš€ **Features:**\n",
    "- Dynamic Intent Detection: Gemini AI âœ“\n",
    "- \"Here\" Location Support: Session Memory âœ“\n",
    "- Location Setting: \"I am in [City]\" âœ“\n",
    "- Max Concurrent Requests: {MAX_CONCURRENT_REQUESTS}\n",
    "- Live Weather API: Open-Meteo (15 days) âœ“\n",
    "- AI Soil Estimation: Gemini AI âœ“\n",
    "- Dynamic Crop Data: Gemini AI âœ“\n",
    "- Contextual Follow-ups: Enabled âœ“\n",
    "\n",
    "ðŸ“Š **System Stats:**\n",
    "- Active Sessions: {stats['total_sessions']}\n",
    "- Currently Processing: {stats['active_requests']} requests\n",
    "- Total Exchanges: {stats['total_exchanges']}\n",
    "- Your Session: {current_session_id[-8:]}\n",
    "\n",
    "ðŸ• Last Updated: {datetime.now().strftime('%H:%M:%S')}\"\"\"\n",
    "\n",
    "def get_conversation_history_display(session_id):\n",
    "    \"\"\"Get formatted conversation history for display for a specific session\"\"\"\n",
    "    conversation_history = session_manager.get_history(session_id)\n",
    "    if not conversation_history:\n",
    "        return f\"No conversation history yet. Start by asking an agriculture-related question!\\n\\n\"\n",
    "    \n",
    "    history_text = f\"## ðŸ“œ Session History (ID: {session_id[-8:]}):\\n\\n\"\n",
    "    \n",
    "    for i, exchange in enumerate(conversation_history[-5:], 1):\n",
    "        timestamp = datetime.fromisoformat(exchange['timestamp']).strftime(\"%H:%M:%S\")\n",
    "        lang_name = SUPPORTED_LANGUAGES.get(exchange['language'], 'Unknown')\n",
    "        q_type = exchange.get('question_type', 'unknown').replace('_', ' ').title()\n",
    "        \n",
    "        # Add special indicators\n",
    "        lang_flag = f\" [{lang_name}]\" if exchange['language'] != 'en' else \"\"\n",
    "        detailed_flag = \" [Expert 400w]\" if exchange.get('is_detailed') else \"\"\n",
    "        followup_flag = \" ðŸ”„\" if q_type.lower() == 'followup' else \"\"\n",
    "        \n",
    "        history_text += f\"**{i}. [{timestamp}]{lang_flag}{detailed_flag}{followup_flag} [{q_type}]**\\n\"\n",
    "        history_text += f\"User: {exchange['user_input']}\\n\"\n",
    "        \n",
    "        if exchange.get('location'):\n",
    "            history_text += f\"ðŸ“ Location: {exchange['location']}\\n\"\n",
    "        \n",
    "        if exchange.get('image_context'):\n",
    "            history_text += f\"ðŸ–¼ï¸ Image: {exchange['image_context']}\\n\"\n",
    "        \n",
    "        response = exchange['bot_response']\n",
    "        if len(response) > 300:\n",
    "            response = response[:300] + \"...\"\n",
    "        \n",
    "        history_text += f\"Assistant: {response}\\n\\n\"\n",
    "    \n",
    "    history_text += \"\\n**Legend:** ðŸ”„ = Query rewritten with history | [Expert 400w] = 400-word Analysis | [Language] = Detected Language\"\n",
    "    \n",
    "    return history_text\n",
    "\n",
    "# ===== GRADIO INTERFACE =====\n",
    "\n",
    "with gr.Blocks(title=\"Enhanced AgroRAG\") as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # ðŸŒ¾ COMPLETE AgroRAG Assistant (Version1 + Version2 Features)\n",
    "    \n",
    "    **All Features Integrated: Expert Analysis â€¢ Non-Agri Filtering â€¢ History Rewriting â€¢ Live Data â€¢ AI Intent Detection**\n",
    "    \n",
    "    ## ðŸš€ Version 1 Features (Integrated):\n",
    "    - ðŸŽ“ **Expert Analysis** - EXACTLY 400 words, 800 tokens detailed consultation\n",
    "    - ðŸš« **Non-Agricultural Filtering** - Politely redirects non-farming questions\n",
    "    - ðŸ”„ **Query Rewriting** - \"Tell me more\" automatically uses conversation history\n",
    "    - ðŸ§  **LLM Intent Analysis** - Detects greetings, agricultural topics, follow-ups\n",
    "    - ðŸ–¼ï¸ **Strict Image Classification** - Only accepts agricultural images\n",
    "    - ðŸ’¬ **Greeting Handling** - Friendly responses to \"Hi\", \"Thanks\", etc.\n",
    "    \n",
    "    ## ðŸš€ Version 2 Features (Integrated):\n",
    "    - ðŸ“ **\"Here\" Location Support** - Set location once, use \"here\" in all questions\n",
    "    - ðŸ’¾ **Session Memory** - Your location saved for the entire session\n",
    "    - âš¡ **5 Concurrent Users** - Simultaneous query processing\n",
    "    - ðŸŒ¦ï¸ **Live Weather Data** - 15-day forecasts from Open-Meteo\n",
    "    - ðŸŒ± **AI Soil Estimation** - Gemini AI-powered soil analysis\n",
    "    - ðŸŒ¾ **Dynamic Crop Database** - Gemini AI-powered requirements\n",
    "    - ðŸ’° **Crop Price Predictions** - Current prices + 3-month forecasts\n",
    "    - ðŸ” **Smart Follow-ups** - Contextual question suggestions\n",
    "    - ðŸŒ **Multilingual** - 13 Indian languages + English\n",
    "    \n",
    "    ## ðŸ’¡ Usage Examples:\n",
    "    **Basic Questions:**\n",
    "    - \"What is crop rotation?\" (gets agricultural answer)\n",
    "    - \"Who won the cricket match?\" (politely redirected)\n",
    "    - \"Hello\" (friendly greeting response)\n",
    "    \n",
    "    **Location-Based:**\n",
    "    - \"I am in Pune\" (sets location)\n",
    "    - \"What is the best crop here?\" (uses saved location)\n",
    "    - \"Best crops for Mumbai\" (gets weather, soil, prices)\n",
    "    \n",
    "    **ðŸ”„ Conversation History & Query Rewriting:**\n",
    "    The system automatically detects follow-up questions and rewrites them using conversation history!\n",
    "    \n",
    "    **Example 1:**\n",
    "    - Ask: \"What is tomato blight?\"\n",
    "    - Then: \"Tell me more about it\" â†’ Auto-rewrites to \"Tell me more about tomato blight\"\n",
    "    - Then: \"How do I prevent this?\" â†’ Auto-rewrites to \"How do I prevent tomato blight?\"\n",
    "    \n",
    "    **Example 2:**\n",
    "    - Ask: \"Best fertilizer for rice?\"\n",
    "    - Then: \"What about the same for wheat?\" â†’ Auto-rewrites with context\n",
    "    \n",
    "    **Example 3:**\n",
    "    - Ask: \"How to treat leaf spot disease?\"\n",
    "    - Then: \"Continue\" â†’ Uses previous topic\n",
    "    \n",
    "    **Triggers for rewriting:** \"it\", \"this\", \"that\", \"tell me more\", \"continue\", \"what about this\", \"the same\"\n",
    "    \n",
    "    **Expert Analysis:**\n",
    "    - Click \"Expert Analysis (400w)\" for comprehensive 400-word consultation\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### ðŸ“ Input Your Question\")\n",
    "            \n",
    "            session_id_input = gr.Textbox(\n",
    "                label=\"Session ID (Optional)\",\n",
    "                placeholder=\"Leave empty for web session\",\n",
    "                visible=True\n",
    "            )\n",
    "            \n",
    "            audio_input = gr.Audio(label=\"ðŸŽ¤ Voice Input\", type=\"numpy\", visible=True)\n",
    "            \n",
    "            transcription_output = gr.Textbox(\n",
    "                label=\"Transcribed Text\",\n",
    "                lines=2,\n",
    "                interactive=True,\n",
    "                placeholder=\"Voice transcription or type directly...\"\n",
    "            )\n",
    "            \n",
    "            text_input = gr.Textbox(\n",
    "                label=\"Or Type Your Question\",\n",
    "                lines=3,\n",
    "                placeholder=\"Try: 'I am in Pune' then 'What is the best crop here?'\"\n",
    "            )\n",
    "            \n",
    "            image_input = gr.Image(label=\"ðŸ“¸ Upload Image (Optional)\", type=\"pil\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                basic_button = gr.Button(\"Get Answer\", variant=\"primary\", size=\"lg\")\n",
    "                expert_button = gr.Button(\"ðŸŽ“ Expert Analysis (400w)\", variant=\"secondary\", size=\"lg\")\n",
    "            \n",
    "            clear_button = gr.Button(\"ðŸ—‘ï¸ Clear All\", variant=\"stop\", size=\"sm\")\n",
    "            \n",
    "        with gr.Column(scale=2):\n",
    "            gr.Markdown(\"### ðŸ¤– Assistant Response\")\n",
    "            \n",
    "            answer_output = gr.Textbox(\n",
    "                label=\"Response\",\n",
    "                lines=25,\n",
    "                interactive=False,\n",
    "                show_copy_button=True\n",
    "            )\n",
    "            \n",
    "            gr.Markdown(\"\"\"\n",
    "            **ðŸ“Š Response includes (Version1 + Version2):**\n",
    "            - **Version1:** LLM intent analysis (agricultural/greeting/follow-up detection)\n",
    "            - **Version1:** Non-agricultural question filtering\n",
    "            - **Version1:** Automatic query rewriting with conversation history\n",
    "            - **Version1:** Expert analysis mode (400 words exactly)\n",
    "            - **Version2:** Real-time weather forecasts (15 days)\n",
    "            - **Version2:** AI-estimated soil analysis\n",
    "            - **Version2:** Dynamic crop suitability scores\n",
    "            - **Version2:** Crop price predictions (current + 3-month forecast)\n",
    "            - **Version2:** Contextual follow-up questions\n",
    "            - **Version2:** Location-specific recommendations\n",
    "            \n",
    "            **ðŸ“‹ Response Types:**\n",
    "            - **Basic Analysis**: Quick, focused answer (150-300 words)\n",
    "            - **Expert Analysis**: Comprehensive consultation (EXACTLY 400 words, 800 tokens)\n",
    "            - **Concurrent**: Multiple users can query simultaneously\n",
    "            - **Thread-Safe**: Isolated session histories\n",
    "            - **Multilingual**: Auto-translates to Indian languages\n",
    "            \"\"\")\n",
    "    \n",
    "    with gr.Accordion(\"ðŸ“œ Your Conversation History\", open=False):\n",
    "        with gr.Row():\n",
    "            refresh_history_btn = gr.Button(\"ðŸ”„ Refresh\", variant=\"secondary\")\n",
    "            clear_history_btn = gr.Button(\"ðŸ—‘ï¸ Clear History\", variant=\"secondary\")\n",
    "        \n",
    "        history_display = gr.Textbox(\n",
    "            label=\"Session History\", \n",
    "            lines=15, \n",
    "            interactive=False,\n",
    "            value=\"No conversation yet.\"\n",
    "        )\n",
    "    \n",
    "    with gr.Accordion(\"âš™ï¸ System Status & API Info\", open=False):\n",
    "        system_status = gr.Textbox(\n",
    "            label=\"System Information\",\n",
    "            lines=20,\n",
    "            interactive=False,\n",
    "            value=f\"\"\"âœ… ENHANCED REAL-TIME DYNAMIC AgroRAG System: Active\n",
    "\n",
    "NEW FEATURES:\n",
    "- Dynamic Intent Detection: Gemini AI\n",
    "- \"Here\" Location Support: Session Memory\n",
    "- Location Setting: \"I am in [City]\"\n",
    "- Max Concurrent: {MAX_CONCURRENT_REQUESTS}\n",
    "- Live Weather: Open-Meteo API (15 days)\n",
    "- AI Soil Analysis: Gemini AI\n",
    "- Dynamic Crops: Gemini AI\n",
    "- Follow-ups: AI-generated\n",
    "\n",
    "Cache TTL: Weather 30min, Soil 60min, Crops 120min\n",
    "\n",
    "API ENDPOINTS:\n",
    "- fetch_session_id_api()\n",
    "- answer_question_with_session(q, img, detailed, sid)\n",
    "- get_conversation_history_by_session(sid)\n",
    "- clear_conversation_history_by_session(sid)\n",
    "- get_similar_questions_api(question, location, topic)\n",
    "- fetch_crop_prices(crop_name, location)\n",
    "\n",
    "Last Updated: {datetime.now().strftime('%H:%M:%S')}\"\"\"\n",
    "        )\n",
    "        \n",
    "        refresh_status_btn = gr.Button(\"ðŸ”„ Refresh Status\")\n",
    "    \n",
    "    with gr.Accordion(\"ðŸ”‘ Session ID Fetcher\", open=False):\n",
    "        fetch_session_btn = gr.Button(\"Get My Session ID\", variant=\"primary\")\n",
    "        session_id_output = gr.JSON(label=\"Session ID Response\")\n",
    "    \n",
    "    # Event Handlers\n",
    "    audio_input.change(fn=transcribe_only, inputs=[audio_input], outputs=[transcription_output])\n",
    "    basic_button.click(\n",
    "        fn=process_basic_question,\n",
    "        inputs=[transcription_output, text_input, image_input, session_id_input],\n",
    "        outputs=[answer_output]\n",
    "    )\n",
    "    expert_button.click(\n",
    "        fn=process_expert_question,\n",
    "        inputs=[transcription_output, text_input, image_input, session_id_input],\n",
    "        outputs=[answer_output]\n",
    "    )\n",
    "    clear_button.click(fn=clear_all_inputs, outputs=[transcription_output, text_input, image_input, answer_output])\n",
    "    refresh_history_btn.click(fn=refresh_history, outputs=[history_display])\n",
    "    clear_history_btn.click(fn=clear_conversation_history, outputs=[history_display])\n",
    "    refresh_status_btn.click(fn=update_system_status, outputs=[system_status])\n",
    "    fetch_session_btn.click(fn=fetch_session_id_api, outputs=[session_id_output])\n",
    "    \n",
    "    # Examples - demonstrating all features including history rewriting\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            [\"What is the best suitable crop in Pune, Maharashtra?\"],  # Version2: Location-based\n",
    "            [\"Which crops can I grow in Varanasi during monsoon season?\"],  # Version2: Location + season\n",
    "            [\"What is crop rotation?\"],  # Version1: Agricultural question\n",
    "            [\"Who won the cricket match?\"],  # Version1: Non-agricultural (will be filtered)\n",
    "            [\"Hello, how can you help me?\"],  # Version1: Greeting handling\n",
    "            [\"Thanks for your help!\"],  # Version1: Thanks handling\n",
    "            [\"What is tomato blight?\"],  # First question for history demo\n",
    "            [\"Tell me more about it\"],  # Version1: History rewriting - will use \"tomato blight\"\n",
    "            [\"How do I prevent this?\"],  # Version1: History rewriting - will use previous context\n",
    "            [\"Continue\"],  # Version1: History rewriting - continuation\n",
    "            [\"What about the same for potato?\"],  # Version1: History rewriting with \"the same\"\n",
    "            [\"I am in Bangalore\"],  # Version2: Location setting\n",
    "            [\"What is the best crop here?\"],  # Version2: Using \"here\" after location set\n",
    "            [\"à¤®à¥à¤‚à¤¬à¤ˆ à¤®à¥‡à¤‚ à¤•à¥Œà¤¨ à¤¸à¥€ à¤«à¤¸à¤² à¤‰à¤—à¤¾à¤à¤‚?\"],  # Multilingual\n",
    "            [\"à¤«à¤¸à¤² à¤•à¥€ à¤¯à¤¹ à¤¬à¥€à¤®à¤¾à¤°à¥€ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\"],  # Hindi - agricultural\n",
    "        ],\n",
    "        inputs=[text_input],\n",
    "        label=\"ðŸ“š Example Questions (Testing All Features Including History Rewriting)\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        demo.launch(\n",
    "            share=True, \n",
    "            server_name=\"0.0.0.0\",\n",
    "            show_api=True,\n",
    "            max_threads=MAX_CONCURRENT_REQUESTS * 2\n",
    "        )\n",
    "    except OSError as e:\n",
    "        if \"Cannot find empty port\" in str(e):\n",
    "            demo.launch(share=True, show_api=True, max_threads=MAX_CONCURRENT_REQUESTS * 2)\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8275330,
     "sourceId": 13067065,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
