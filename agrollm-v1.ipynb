{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12596615,"sourceType":"datasetVersion","datasetId":7956112}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q gradio transformers accelerate langchain langchain-community faiss-cpu sentence-transformers pymupdf SpeechRecognition pyttsx3 deep_translator langdetect langchain_huggingface","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-19T06:38:11.609245Z","iopub.execute_input":"2025-08-19T06:38:11.609886Z","iopub.status.idle":"2025-08-19T06:39:38.234513Z","shell.execute_reply.started":"2025-08-19T06:38:11.609836Z","shell.execute_reply":"2025-08-19T06:39:38.233806Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m443.5/443.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pickle\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom langchain_community.llms import HuggingFacePipeline\nfrom langchain.vectorstores import FAISS\nfrom langchain.document_loaders import PyMuPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\n\n# Paths\nDATA_FOLDER = \"/kaggle/input/agrollm-data/agrollm_dataset\"\nINDEX_DIR = \"/kaggle/working/AgroRAG/faiss_index\"\nCHUNKS_PATH = \"/kaggle/working/AgroRAG/chunks.pkl\"\n\n# 1. Load PDFs from subfolders\ndef load_documents_from_folders(root_dir):\n    all_docs = []\n    for subdir, _, files in os.walk(root_dir):\n        for file in files:\n            if file.endswith(\".pdf\"):\n                path = os.path.join(subdir, file)\n                print(f\"Loading {path}...\")\n                loader = PyMuPDFLoader(path)\n                docs = loader.load()\n                for doc in docs:\n                    doc.metadata['source'] = path\n                    doc.metadata['topic'] = os.path.basename(subdir)\n                all_docs.extend(docs)\n    return all_docs\n\n# 2. Split text into chunks\ndef split_documents(documents, chunk_size=500, chunk_overlap=50):\n    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n    return splitter.split_documents(documents)\n\n# 3. Embed and store\ndef build_or_load_faiss_index(chunks, index_path=INDEX_DIR):\n    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n    if os.path.exists(os.path.join(index_path, \"index.faiss\")):\n        print(\"Loading existing FAISS index...\")\n        return FAISS.load_local(index_path, embedder)\n    print(\"Creating new FAISS index...\")\n    vectordb = FAISS.from_documents(chunks, embedder)\n    os.makedirs(index_path, exist_ok=True)\n    vectordb.save_local(index_path)\n    return vectordb\n\n# 4. Load Mistral-7B-Instruct for Local RAG\ndef load_local_llm():\n    model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, temperature=0.7)\n    return HuggingFacePipeline(pipeline=pipe)\n\n# 5. RAG QA Chain with Local Model\ndef run_qa_chain(vectorstore, question):\n    llm = load_local_llm()\n    retriever = vectorstore.as_retriever(search_type=\"similarity\", k=4)\n    qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n    return qa.run(question)\n\n# Main Pipeline\nif __name__ == \"__main__\":\n    if os.path.exists(CHUNKS_PATH):\n        print(\"âœ… Loading pre-saved chunks...\")\n        with open(CHUNKS_PATH, 'rb') as f:\n            chunks = pickle.load(f)\n    else:\n        print(\"ğŸ“„ Loading and chunking documents...\")\n        docs = load_documents_from_folders(DATA_FOLDER)\n        chunks = split_documents(docs)\n        os.makedirs(os.path.dirname(CHUNKS_PATH), exist_ok=True)\n        with open(CHUNKS_PATH, 'wb') as f:\n            pickle.dump(chunks, f)\n\n    print(\"ğŸ§  Building or loading FAISS index...\")\n    db = build_or_load_faiss_index(chunks)\n\n    print(\"\\nâœ… Chunking and indexing complete. You can now use the inference script to ask questions.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T15:00:22.315312Z","iopub.execute_input":"2025-08-12T15:00:22.315568Z","iopub.status.idle":"2025-08-12T15:01:25.315178Z","shell.execute_reply.started":"2025-08-12T15:00:22.315543Z","shell.execute_reply":"2025-08-12T15:01:25.314481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport gradio as gr\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom transformers import BlipProcessor, BlipForConditionalGeneration, ViTImageProcessor, ViTForImageClassification\nfrom langchain_community.llms import HuggingFacePipeline\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom huggingface_hub import login\nimport speech_recognition as sr\nimport numpy as np\nfrom PIL import Image\nimport gc\nimport io\nimport wave\nimport tempfile\nimport json\nfrom datetime import datetime\n\n# NEW: Multilingual imports\nimport re\nimport warnings\nfrom deep_translator import GoogleTranslator\nfrom langdetect import detect\n\n# Login to Hugging Face\nlogin(\"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\")\n\n# OPTIMIZATION: Set optimal GPU settings for P100\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\n# Global variables\nqa_chain = None\nclassifier_pipeline = None\nyour_llm_pipeline = None\nrecognizer = sr.Recognizer()\nblip_model = None\nblip_processor = None\nvit_model = None\nvit_processor = None\nINDEX_PATH = \"/kaggle/working/AgroRAG/faiss_index\"\n\n# NEW: Global conversation history for session-based memory\nconversation_history = []\nMAX_HISTORY_LENGTH = 10  # Keep last 10 exchanges\n\n# NEW: Multilingual utility functions\ndef detect_language(text):\n    \"\"\"Detect language of input text\"\"\"\n    try:\n        return detect(text)\n    except:\n        return 'en'  # Default to English if detection fails\n\ndef translate_to_english(text):\n    \"\"\"Translate text to English if it's not already in English\"\"\"\n    lang = detect_language(text)\n    if lang == 'en':\n        return text, lang\n    try:\n        translated = GoogleTranslator(source=lang, target='en').translate(text)\n        return translated, lang\n    except:\n        return text, lang  # Return original if translation fails\n\ndef translate_from_english(text, target_lang):\n    \"\"\"Translate text from English to target language\"\"\"\n    if target_lang == 'en':\n        return text\n    try:\n        return GoogleTranslator(source='en', target=target_lang).translate(text)\n    except:\n        return text  # Return original if translation fails\n\ndef clean_answer(text):\n    \"\"\"Clean answer for better display\"\"\"\n    text = re.sub(r\"https?://\\S+\", \"\", text)             # remove URLs\n    text = re.sub(r\"\\[[^\\]]*\\]\", \"\", text)               # remove citations\n    text = re.sub(r\"\\([^)]*\\)\", \"\", text)                # remove parenthesis\n    text = re.sub(r\"\\bn\\d+\\b\", \"\", text)                 # remove n1/n2\n    text = re.sub(r\"\\s{2,}\", \" \", text)                  # remove extra spaces\n    return text.strip()\n\ndef cleanup_memory():\n    \"\"\"Clean up GPU memory\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n\n# NEW: Function to add conversation to history\ndef add_to_history(user_input, bot_response, image_context=\"\", original_lang=\"en\"):\n    \"\"\"Add conversation exchange to history with language info\"\"\"\n    global conversation_history\n    \n    exchange = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"user_input\": user_input,\n        \"bot_response\": bot_response,\n        \"image_context\": image_context,\n        \"language\": original_lang  # NEW: Track original language\n    }\n    \n    conversation_history.append(exchange)\n    \n    # Keep only recent history to manage memory\n    if len(conversation_history) > MAX_HISTORY_LENGTH:\n        conversation_history = conversation_history[-MAX_HISTORY_LENGTH:]\n\n# NEW: Function to get conversation context\ndef get_conversation_context():\n    \"\"\"Get formatted conversation history for context\"\"\"\n    if not conversation_history:\n        return \"\"\n    \n    context = \"\\nRecent conversation history:\\n\"\n    for i, exchange in enumerate(conversation_history[-3:], 1):  # Last 3 exchanges\n        context += f\"{i}. User: {exchange['user_input']}\\n\"\n        context += f\"   Assistant: {exchange['bot_response'][:150]}...\\n\"\n        if exchange.get('image_context'):\n            context += f\"   Image: {exchange['image_context']}\\n\"\n    \n    return context\n\n# NEW: Function to check for interactive/greeting inputs\ndef is_interactive_input(text):\n    \"\"\"Check if input is a greeting or interactive message\"\"\"\n    if not text or len(text.strip()) < 2:\n        return False\n    \n    text_lower = text.lower().strip()\n    \n    # Greeting patterns\n    greetings = [\n        'hi', 'hello', 'hey', 'good morning', 'good afternoon', 'good evening',\n        'greetings', 'howdy', 'hiya', 'sup', 'what\\'s up', 'whats up'\n    ]\n    \n    # Interactive patterns\n    interactive_patterns = [\n        'how are you', 'how do you do', 'nice to meet you', 'pleasure to meet you',\n        'thanks', 'thank you', 'bye', 'goodbye', 'see you', 'take care',\n        'help me', 'can you help', 'what can you do', 'what do you do'\n    ]\n    \n    # Check exact matches for greetings\n    if text_lower in greetings:\n        return True\n    \n    # Check if any interactive pattern is in the text\n    for pattern in interactive_patterns:\n        if pattern in text_lower:\n            return True\n    \n    return False\n\n# NEW: Function to generate interactive responses\ndef generate_interactive_response(text):\n    \"\"\"Generate appropriate interactive response\"\"\"\n    text_lower = text.lower().strip()\n    \n    # Greeting responses\n    if any(greeting in text_lower for greeting in ['hi', 'hello', 'hey', 'good morning', 'good afternoon', 'good evening', 'greetings', 'howdy', 'hiya']):\n        responses = [\n            \"Hello! ğŸ‘‹ I'm your AgroRAG assistant, ready to help you with all your agriculture-related questions. How can I assist you today?\",\n            \"Hi there! ğŸŒ± I'm here to help you with farming, crops, livestock, and all things agriculture. What would you like to know?\",\n            \"Greetings! ğŸšœ I'm your agricultural expert assistant. Feel free to ask me about crops, soil, pests, diseases, or any farming topics!\",\n            \"Hello! ğŸŒ¾ Welcome to AgroRAG. I'm specialized in agriculture and farming advice. What agricultural topic can I help you with today?\"\n        ]\n        import random\n        return random.choice(responses)\n    \n    # How are you responses\n    elif any(phrase in text_lower for phrase in ['how are you', 'how do you do']):\n        return \"I'm doing great, thank you for asking! ğŸ˜Š I'm ready and excited to help you with any agriculture-related questions. What farming or crop topic would you like to explore today?\"\n    \n    # Thank you responses\n    elif any(phrase in text_lower for phrase in ['thanks', 'thank you']):\n        return \"You're very welcome! ğŸŒŸ I'm always happy to help with agriculture and farming questions. Feel free to ask me anything else about crops, livestock, soil, or farming techniques!\"\n    \n    # Goodbye responses\n    elif any(phrase in text_lower for phrase in ['bye', 'goodbye', 'see you', 'take care']):\n        return \"Goodbye! ğŸ‘‹ Take care and happy farming! Feel free to come back anytime you have agriculture-related questions. Have a great day! ğŸŒ±\"\n    \n    # Help/capability responses\n    elif any(phrase in text_lower for phrase in ['help me', 'can you help', 'what can you do', 'what do you do']):\n        return \"\"\"I'm your specialized agriculture assistant! ğŸŒ¾ Here's how I can help you:\n\nğŸŒ± *Crop Management*: Planting, growing, harvesting advice\nğŸ¦  *Disease & Pest Control*: Identification and treatment solutions  \nğŸŒ *Soil & Fertilization*: Soil health, nutrients, fertilizer recommendations\nğŸ’§ *Irrigation*: Water management and irrigation techniques\nğŸ„ *Livestock Care*: Animal husbandry and livestock management\nğŸŒ¿ *Organic Farming*: Sustainable and organic farming practices\nğŸšœ *Equipment & Tools*: Agricultural machinery and tool guidance\nğŸ“Š *Farm Planning*: Crop rotation, seasonal planning, farm economics\n\nYou can ask me questions, upload images of plants or crops for analysis, or even use voice input! What agricultural topic interests you today?\"\"\"\n    \n    # Default friendly response\n    else:\n        return \"Hello! ğŸ˜Š I'm here to help you with agriculture and farming questions. Whether you want to know about crops, soil, livestock, or farming techniques, I'm ready to assist! What would you like to learn about today? ğŸŒ±\"\n\n# NEW: Function to check if question relates to conversation history\ndef relates_to_conversation_history(current_input):\n    \"\"\"Check if current input relates to previous conversation\"\"\"\n    if not conversation_history:\n        return False, \"\"\n    \n    # Keywords that suggest reference to previous conversation\n    reference_keywords = [\n        'this', 'that', 'it', 'more about', 'tell me more', 'expand on',\n        'details about', 'explain further', 'what about', 'regarding',\n        'continue', 'elaborate', 'additional', 'furthermore'\n    ]\n    \n    current_lower = current_input.lower()\n    \n    # Check if any reference keywords are present\n    has_reference = any(keyword in current_lower for keyword in reference_keywords)\n    \n    if has_reference and len(conversation_history) > 0:\n        # Get the last exchange for context\n        last_exchange = conversation_history[-1]\n        context = f\"Previous question: {last_exchange['user_input']}\\nPrevious answer: {last_exchange['bot_response'][:200]}...\"\n        return True, context\n    \n    return False, \"\"\n\ndef initialize_speech_recognition():\n    \"\"\"Initialize speech recognition\"\"\"\n    global recognizer\n    \n    try:\n        recognizer = sr.Recognizer()\n        # Adjust for ambient noise\n        recognizer.energy_threshold = 4000\n        recognizer.dynamic_energy_threshold = True\n        print(\"âœ… Speech Recognition initialized successfully\")\n        return True\n        \n    except Exception as e:\n        print(f\"âŒ Error initializing speech recognition: {e}\")\n        return False\n\ndef initialize_image_models():\n    \"\"\"Initialize optimized image models\"\"\"\n    global blip_model, blip_processor, vit_model, vit_processor\n    \n    try:\n        print(\"Loading optimized BLIP model...\")\n        blip_model_id = \"Salesforce/blip-image-captioning-base\"\n        blip_processor = BlipProcessor.from_pretrained(blip_model_id)\n        blip_model = BlipForConditionalGeneration.from_pretrained(\n            blip_model_id,\n            torch_dtype=torch.float16,\n            device_map=\"cuda:0\"\n        ).eval()\n        print(\"âœ… BLIP model loaded and optimized\")\n        \n        print(\"Loading crop disease ViT model...\")\n        vit_model_id = \"wambugu71/crop_leaf_diseases_vit\"\n        vit_processor = ViTImageProcessor.from_pretrained(vit_model_id)\n        vit_model = ViTForImageClassification.from_pretrained(\n            vit_model_id,\n            torch_dtype=torch.float16,\n            device_map=\"cuda:0\"\n        ).eval()\n        print(\"âœ… ViT model loaded and optimized\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"âŒ Failed to load image models: {e}\")\n        return False\n\n@torch.inference_mode()  # OPTIMIZATION: Disable gradient computation\ndef analyze_uploaded_image(image):\n    \"\"\"OPTIMIZED image analysis with faster inference\"\"\"\n    global blip_model, blip_processor, vit_model, vit_processor\n    \n    if image is None:\n        return \"\", \"\"\n    \n    if blip_model is None or vit_model is None:\n        return \"Image models not initialized\", \"Unknown\"\n    \n    try:\n        # Ensure image is in PIL format and resize for faster processing\n        if not isinstance(image, Image.Image):\n            image = Image.fromarray(image).convert('RGB')\n        else:\n            image = image.convert('RGB')\n        \n        # OPTIMIZATION: Resize image to reduce processing time\n        image = image.resize((224, 224), Image.Resampling.LANCZOS)\n        \n        # BLIP Captioning with optimizations\n        blip_inputs = blip_processor(images=image, return_tensors=\"pt\")\n        blip_inputs = {k: v.to(\"cuda:0\", dtype=torch.float16) for k, v in blip_inputs.items()}\n        \n        caption_ids = blip_model.generate(\n            **blip_inputs, \n            max_new_tokens=20,  # Reduced from 30\n            num_beams=2,        # Reduced from default 5\n            do_sample=False,\n            early_stopping=True\n        )\n        caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n        \n        # ViT Classification with optimizations\n        vit_inputs = vit_processor(images=image, return_tensors=\"pt\")\n        vit_inputs = {k: v.to(\"cuda:0\", dtype=torch.float16) for k, v in vit_inputs.items()}\n        \n        outputs = vit_model(**vit_inputs)\n        label_idx = outputs.logits.argmax(-1).item()\n        label = vit_model.config.id2label[label_idx]\n        \n        print(f\"Fast analysis - Caption: {caption}, Disease: {label}\")\n        return caption, label\n        \n    except Exception as e:\n        print(f\"Error in image analysis: {e}\")\n        return \"Error analyzing image\", \"Error in classification\"\n\ndef transcribe_audio(audio):\n    \"\"\"Audio transcription using speech_recognition library\"\"\"\n    global recognizer\n    \n    if recognizer is None:\n        return \"âŒ Speech recognition not initialized\"\n    \n    if audio is None:\n        return \"âŒ No audio provided\"\n    \n    try:\n        sample_rate, audio_data = audio\n        \n        # Convert numpy array to audio data that speech_recognition can use\n        if audio_data.dtype == np.int16:\n            audio_data = audio_data.astype(np.float32) / 32768.0\n        elif audio_data.dtype == np.int32:\n            audio_data = audio_data.astype(np.float32) / 2147483648.0\n        else:\n            audio_data = audio_data.astype(np.float32)\n        \n        # Convert to 16-bit PCM\n        audio_data = (audio_data * 32767).astype(np.int16)\n        \n        # Create a temporary wav file\n        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_file:\n            # Write WAV file\n            with wave.open(tmp_file.name, 'wb') as wav_file:\n                wav_file.setnchannels(1 if len(audio_data.shape) == 1 else audio_data.shape[1])\n                wav_file.setsampwidth(2)  # 16-bit\n                wav_file.setframerate(sample_rate)\n                wav_file.writeframes(audio_data.tobytes())\n            \n            # Use speech_recognition to transcribe\n            with sr.AudioFile(tmp_file.name) as source:\n                # Adjust for ambient noise\n                recognizer.adjust_for_ambient_noise(source, duration=0.5)\n                audio = recognizer.record(source)\n                \n                try:\n                    # Use Google Web Speech API (free)\n                    text = recognizer.recognize_google(audio)\n                    return text\n                except sr.UnknownValueError:\n                    return \"âŒ Could not understand audio\"\n                except sr.RequestError as e:\n                    # Fallback to offline recognition if available\n                    try:\n                        text = recognizer.recognize_sphinx(audio)\n                        return text\n                    except:\n                        return f\"âŒ Speech recognition service error: {e}\"\n        \n        # Clean up temporary file\n        try:\n            os.unlink(tmp_file.name)\n        except:\n            pass\n            \n    except Exception as e:\n        print(f\"Error in transcription: {e}\")\n        return f\"âŒ Error transcribing audio: {str(e)}\"\n\ndef initialize_classifier():\n    \"\"\"Use the main Mistral model for classification\"\"\"\n    global classifier_pipeline\n    \n    print(\"âœ… Using main Mistral 7B model for LLM-based classification (shared with QA)\")\n    # We'll use the same pipeline as the QA system for classification\n    classifier_pipeline = \"mistral_shared\"  # Flag to indicate we're using the shared model\n    return True \n\ndef classify_question_with_llm(question, conversation_context=\"\"):\n    \"\"\"Pure LLM-based classification for accurate context understanding - MODIFIED to include conversation context\"\"\"\n    global your_llm_pipeline\n    \n    if your_llm_pipeline is None:\n        print(\"Warning: LLM not available for classification, defaulting to allow\")\n        return True  # Default to allowing questions when LLM not available\n    \n    try:\n        # Clean and prepare the question\n        question_clean = question.strip()\n        if len(question_clean) < 3:\n            return False\n        \n        # Create a detailed classification prompt for LLM - MODIFIED to include conversation context\n        classification_prompt = f\"\"\"<s>[INST] You are an expert agriculture classifier. Your task is to determine if the given question or statement is related to agriculture, farming, crops, livestock, plants, or rural/agricultural topics.\n\nAGRICULTURE-RELATED TOPICS INCLUDE:\n- Crop cultivation, planting, harvesting\n- Plant diseases, pests, plant health\n- Soil management, fertilizers, nutrients\n- Irrigation, water management\n- Livestock care, animal husbandry\n- Organic farming, sustainable agriculture\n- Agricultural tools, equipment, machinery\n- Farm management, agricultural economics\n- Plant biology, botany (in agricultural context)\n- Weather/climate effects on farming\n- Food production and processing\n- Gardening and horticulture\n\nNON-AGRICULTURE TOPICS INCLUDE:\n- General technology, computers, software\n- Medicine, human health (unless related to farm safety)\n- General science (unless agricultural science)\n- Entertainment, sports, politics\n- General education topics\n- Urban planning, city topics\n\nIMPORTANT: If the current question refers to previous conversation about agriculture (using words like \"this\", \"that\", \"more about it\", \"tell me more\", etc.), and the conversation history shows agriculture-related topics, then classify it as agriculture-related.\n\n{conversation_context}\n\nCurrent question to classify: \"{question_clean}\"\n\nBased on the question above and any conversation history, is this about agriculture, farming, plants, or related rural topics?\n\nAnswer with only \"YES\" if it's agriculture-related, or \"NO\" if it's not agriculture-related.\n\nClassification: [/INST]\"\"\"\n\n        print(f\"ğŸ§  Using LLM to classify: '{question_clean[:50]}...'\")\n        \n        # Get classification result from LLM\n        try:\n            result = your_llm_pipeline(\n                classification_prompt,\n                max_new_tokens=5,  # Just need YES/NO\n                do_sample=False,\n                temperature=0.1,\n                pad_token_id=your_llm_pipeline.tokenizer.eos_token_id\n            )\n            \n            if result and len(result) > 0:\n                response = result[0]['generated_text'].strip().upper()\n                print(f\"ğŸ¤– LLM Classification result: {response}\")\n                \n                # Check for positive indicators\n                is_agriculture = \"YES\" in response or \"AGRICULTURE\" in response or \"FARMING\" in response\n                print(f\"âœ… Final classification: {is_agriculture}\")\n                return is_agriculture\n                        \n        except Exception as e:\n            print(f\"âŒ LLM classification error: {e}\")\n            # If LLM fails, default to allowing (better user experience)\n            print(\"âš  LLM classification failed, defaulting to ALLOW\")\n            return True\n        \n        # If we get here, something went wrong - default to allowing\n        return True\n        \n    except Exception as e:\n        print(f\"âŒ Classification error: {e}\")\n        return True  # Default to allowing questions\n\ndef classify_image_with_llm(image_caption, classification_label):\n    \"\"\"Pure LLM-based image classification\"\"\"\n    global your_llm_pipeline\n    \n    if your_llm_pipeline is None:\n        print(\"Warning: LLM not available for image classification, defaulting to allow\")\n        return True\n    \n    if not image_caption and not classification_label:\n        print(\"No image context available\")\n        return False\n    \n    try:\n        # Combine image analysis results\n        image_context = f\"Image caption: {image_caption}\\nImage classification: {classification_label}\"\n        \n        # Create LLM prompt for image classification\n        image_classification_prompt = f\"\"\"<s>[INST] You are an expert agriculture classifier analyzing image descriptions. Your task is to determine if the described image is related to agriculture, farming, crops, livestock, plants, or rural/agricultural topics.\n\nAGRICULTURE-RELATED IMAGES INCLUDE:\n- Plants, leaves, flowers, fruits, vegetables\n- Crops in fields, gardens, greenhouses  \n- Plant diseases, pests, plant health issues\n- Farm animals, livestock\n- Agricultural tools, machinery, equipment\n- Soil, farming landscapes, rural settings\n- Seeds, grains, harvest scenes\n- Botanical specimens in agricultural context\n\nNON-AGRICULTURE IMAGES INCLUDE:\n- Indoor objects, furniture, electronics\n- Urban scenes, buildings, roads\n- Prepared food, cooked meals, kitchen scenes\n- People in non-agricultural settings\n- Vehicles (unless farm equipment)\n- General household items\n- Technology devices, computers\n\nImage description to analyze:\n{image_context}\n\nBased on the image description above, does this image show agriculture, farming, plants, crops, or related rural content?\n\nAnswer with only \"YES\" if it shows agriculture-related content, or \"NO\" if it does not.\n\nClassification: [/INST]\"\"\"\n\n        print(f\"ğŸ–¼ Using LLM to classify image: {image_context[:100]}...\")\n        \n        try:\n            result = your_llm_pipeline(\n                image_classification_prompt,\n                max_new_tokens=5,\n                do_sample=False,\n                temperature=0.1,\n                pad_token_id=your_llm_pipeline.tokenizer.eos_token_id\n            )\n            \n            if result and len(result) > 0:\n                response = result[0]['generated_text'].strip().upper()\n                print(f\"ğŸ¤– LLM Image Classification result: {response}\")\n                \n                is_agriculture = \"YES\" in response or \"AGRICULTURE\" in response or \"FARMING\" in response\n                print(f\"âœ… Final image classification: {is_agriculture}\")\n                return is_agriculture\n                        \n        except Exception as e:\n            print(f\"âŒ LLM image classification error: {e}\")\n            return True  # Default to allowing\n        \n        return True\n        \n    except Exception as e:\n        print(f\"âŒ Image classification error: {e}\")\n        return True\n\ndef load_vectorstore(index_path):\n    \"\"\"Load FAISS vectorstore with optimizations\"\"\"\n    try:\n        embedder = HuggingFaceEmbeddings(\n            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n            model_kwargs={'device': 'cuda:0'}  # Force embeddings to GPU\n        )\n        vectorstore = FAISS.load_local(index_path, embedder, allow_dangerous_deserialization=True)\n        print(\"âœ… FAISS vectorstore loaded with GPU embeddings\")\n        return vectorstore\n    except Exception as e:\n        print(f\"Error loading vectorstore: {e}\")\n        return None\n\ndef build_qa_chain(vectorstore):\n    \"\"\"Build OPTIMIZED QA chain\"\"\"\n    global your_llm_pipeline\n    \n    try:\n        # OPTIMIZATION: Use smaller Mistral model or consider alternatives\n        model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n        \n        tokenizer = AutoTokenizer.from_pretrained(model_id)\n        tokenizer.pad_token = tokenizer.eos_token\n        \n        print(\"Loading Mistral model with optimizations...\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16,\n            device_map=\"cuda:0\",\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n            use_cache=True  # Enable KV cache for faster generation\n        ).eval()\n        \n        # OPTIMIZATION: Faster pipeline settings\n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            max_new_tokens=256,    # Reduced from 512\n            temperature=0.1,       # Lower temperature for faster, more focused responses\n            do_sample=False,       # Disable sampling for speed\n            return_full_text=False,\n            pad_token_id=tokenizer.eos_token_id,\n            batch_size=1\n        )\n        \n        your_llm_pipeline = pipe\n        llm = HuggingFacePipeline(pipeline=pipe)\n        \n        # OPTIMIZATION: Reduce retrieval count\n        retriever = vectorstore.as_retriever(search_type=\"similarity\", k=2)  # Reduced from 4\n        \n        return RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n    \n    except Exception as e:\n        print(f\"Error building QA chain: {e}\")\n        return None\n\ndef initialize_system():\n    \"\"\"Initialize the optimized system\"\"\"\n    global qa_chain\n    \n    try:\n        cleanup_memory()  # Start with clean memory\n        \n        print(\"Initializing speech recognition...\")\n        speech_success = initialize_speech_recognition()\n        if not speech_success:\n            print(\"Warning: Speech recognition initialization failed\")\n        \n        print(\"Initializing optimized image models...\")\n        image_success = initialize_image_models()\n        if not image_success:\n            print(\"Warning: Image models failed to load\")\n        \n        print(\"Initializing LLM-based classifier...\")\n        classifier_success = initialize_classifier()\n        \n        if not os.path.exists(INDEX_PATH):\n            return \"Error: FAISS index not found\"\n            \n        print(\"Loading optimized FAISS index...\")\n        vectordb = load_vectorstore(INDEX_PATH)\n        \n        if vectordb is None:\n            return \"Error: Failed to load FAISS index\"\n        \n        print(\"Loading optimized Mistral model...\")\n        qa_chain = build_qa_chain(vectordb)\n        \n        if qa_chain is None:\n            return \"Error: Failed to build QA chain\"\n        \n        cleanup_memory()  # Clean up after initialization\n        \n        return \"ğŸš€ OPTIMIZED System ready! Speech recognition, image analysis, conversational memory, interactive responses, and multilingual support initialized.\"\n            \n    except Exception as e:\n        return f\"Error initializing system: {str(e)}\"\n\ndef review_and_refine_answer(initial_answer, original_question, image_context=\"\", conversation_context=\"\"):\n    \"\"\"Review and refine the initial answer using Mistral 7B - MODIFIED to include conversation context\"\"\"\n    global your_llm_pipeline\n    \n    if your_llm_pipeline is None:\n        print(\"Warning: Mistral model not available for review, returning original answer\")\n        return initial_answer\n    \n    try:\n        # Create a comprehensive review prompt - MODIFIED to include conversation context\n        review_prompt = f\"\"\"<s>[INST] You are an expert agricultural consultant reviewing an AI-generated answer for accuracy, completeness, and clarity.\n\n{conversation_context}\n\nOriginal Question: \"{original_question}\"\n{f\"Image Context: {image_context}\" if image_context else \"\"}\n\nInitial AI Answer:\n{initial_answer}\n\nYour task is to review this answer and improve it by:\n\n1. *ACCURACY CHECK*: Verify all agricultural/botanical facts are correct\n2. *COMPLETENESS*: Add any important missing information\n3. *CLARITY*: Make the explanation clearer and more structured\n4. *PRACTICAL VALUE*: Include actionable advice when appropriate\n5. *SAFETY*: Ensure recommendations are safe and environmentally sound\n6. *CONTEXT AWARENESS*: Consider the conversation history when refining the answer\n\nGuidelines for review:\n- If the answer is factually incorrect, correct it\n- If important information is missing, add it\n- If the answer is unclear, restructure it\n- If there are contradictions, resolve them\n- If the answer seems incomplete, expand it appropriately\n- Keep the tone professional but accessible\n- Focus on practical, actionable information\n- If this question relates to previous conversation, acknowledge that connection\n\nProvide the REVISED AND IMPROVED answer: [/INST]\"\"\"\n\n        print(\"ğŸ” Reviewing and refining the initial answer...\")\n        \n        # Get refined answer from Mistral\n        refined_result = your_llm_pipeline(\n            review_prompt,\n            max_new_tokens=400,  # Allow more tokens for comprehensive review\n            temperature=0.2,     # Slightly higher for more natural refinement\n            do_sample=False,\n            return_full_text=False\n        )\n        \n        if refined_result and len(refined_result) > 0:\n            refined_answer = refined_result[0]['generated_text'].strip()\n            \n            # Basic quality check on refined answer\n            if len(refined_answer) > 50 and len(refined_answer) < 3000:  # Reasonable length\n                print(\"âœ… Answer successfully reviewed and refined\")\n                return refined_answer\n            else:\n                print(\"âš  Refined answer seems problematic, using original\")\n                return initial_answer\n        else:\n            print(\"âš  Review process failed, using original answer\")\n            return initial_answer\n            \n    except Exception as e:\n        print(f\"âŒ Error in review process: {e}\")\n        return initial_answer\n\ndef answer_question(question, image=None):\n    \"\"\"OPTIMIZED answer function with PURE LLM-based classification, conversation history, interactive responses, and MULTILINGUAL support\"\"\"\n    global qa_chain\n    \n    if qa_chain is None:\n        return \"âŒ System not initialized\"\n    \n    if not question.strip() and image is None:\n        return \"â“ Please enter a question or upload an image\"\n        \n    try:\n        question_text = question.strip()\n        original_lang = \"en\"  # Default language\n        english_question = question_text\n        \n        # NEW: Language detection and translation\n        if question_text:\n            english_question, original_lang = translate_to_english(question_text)\n            if original_lang != \"en\":\n                print(f\"ğŸŒ Detected language: {original_lang}, translated to English for processing\")\n        \n        # NEW: Check for interactive/greeting inputs first (use English version for consistency)\n        if english_question and is_interactive_input(english_question):\n            print(\"ğŸ¯ Interactive input detected\")\n            interactive_response = generate_interactive_response(english_question)\n            \n            # NEW: Translate interactive response back to original language\n            if original_lang != \"en\":\n                interactive_response = translate_from_english(interactive_response, original_lang)\n                print(f\"ğŸŒ Interactive response translated back to {original_lang}\")\n            \n            # Add to conversation history\n            add_to_history(question_text, interactive_response, \"\", original_lang)\n            return interactive_response\n        \n        # Fast image analysis\n        image_caption = \"\"\n        classification_label = \"\"\n        if image is not None:\n            image_caption, classification_label = analyze_uploaded_image(image)\n\n        # NEW: Check if question relates to conversation history\n        relates_to_history, history_context = relates_to_conversation_history(english_question)\n        conversation_context = get_conversation_context()\n        \n        print(f\"ğŸ”— Relates to conversation history: {relates_to_history}\")\n        \n        # PURE LLM-BASED CLASSIFICATION - No rule-based fallbacks\n        is_agriculture = False\n        \n        if image is not None:\n            print(\"ğŸ–¼ Image uploaded - using PURE LLM-based classification\")\n            \n            # Use LLM to classify image content\n            image_is_agriculture = classify_image_with_llm(image_caption, classification_label)\n            print(f\"ğŸ¤– LLM Image classification: {image_is_agriculture}\")\n            \n            if english_question:\n                # Use LLM to classify the question (with conversation context)\n                question_is_agriculture = classify_question_with_llm(english_question, conversation_context)\n                print(f\"ğŸ¤– LLM Question classification: {question_is_agriculture}\")\n                \n                # Allow if EITHER question OR image is agriculture-related (OR logic)\n                if question_is_agriculture or image_is_agriculture:\n                    is_agriculture = True\n                    if question_is_agriculture and image_is_agriculture:\n                        print(\"âœ… LLM Decision: Both question and image are agriculture-related\")\n                    elif question_is_agriculture and not image_is_agriculture:\n                        print(\"âœ… LLM Decision: Question is agriculture-related (image is not)\")\n                    elif not question_is_agriculture and image_is_agriculture:\n                        print(\"âœ… LLM Decision: Image is agriculture-related (question is not)\")\n                else:\n                    is_agriculture = False\n                    print(\"âŒ LLM Decision: Neither question nor image is agriculture-related\")\n            else:\n                # No question provided, just check image with LLM\n                if image_is_agriculture:\n                    is_agriculture = True\n                    print(\"âœ… LLM Decision: No question provided but image is agriculture-related\")\n                else:\n                    is_agriculture = False\n                    print(\"âŒ LLM Decision: No question provided and image is not agriculture-related\")\n                \n        else:\n            # No image, just classify the question with LLM (including conversation context)\n            is_agriculture = classify_question_with_llm(english_question, conversation_context)\n            print(f\"ğŸ¤– LLM Text-only classification: {is_agriculture}\")\n\n        if not is_agriculture:\n            non_agriculture_response = \"\"\"ğŸš« *Agriculture Classification Result*:\n\nBased on my AI analysis, your question/image doesn't appear to be related to agriculture, farming, crops, or livestock.\n\nI'm specifically designed to help with agriculture-related topics such as:\n- Crop cultivation and management\n- Soil health and fertilization  \n- Irrigation and water management\n- Pest and disease control\n- Livestock care and management\n- Organic farming practices\n- Agricultural equipment and techniques\n- Plant biology and botany\n- Agricultural economics and planning\n\nPlease ask a question related to agriculture or upload an agriculture-related image, and I'll be happy to help! ğŸŒ±\"\"\"\n            \n            # NEW: Translate non-agriculture response back to original language\n            if original_lang != \"en\":\n                non_agriculture_response = translate_from_english(non_agriculture_response, original_lang)\n                print(f\"ğŸŒ Non-agriculture response translated back to {original_lang}\")\n            \n            # Add to conversation history even for non-agriculture responses\n            add_to_history(question_text, non_agriculture_response, \n                         f\"{image_caption} - {classification_label}\" if image else \"\", original_lang)\n            return non_agriculture_response\n        \n        # Combine context for RAG (use English for RAG processing)\n        full_context = \"\"\n        if image_caption or classification_label:\n            full_context += f\"Image: {image_caption}\\nIssue: {classification_label}\\n\"\n        \n        # NEW: Add conversation context if available\n        if relates_to_history:\n            full_context += f\"Previous context: {history_context}\\n\"\n        \n        full_context += f\"Question: {english_question}\"\n        \n        print(\"ğŸ¤– Generating initial answer from RAG system...\")\n        \n        # STEP 1: Get initial RAG response (in English)\n        initial_result = qa_chain.run(full_context)\n        \n        print(\"ğŸ” Reviewing and refining the answer...\")\n        \n        # STEP 2: Review and refine the answer (in English)\n        image_context = \"\"\n        if image_caption or classification_label:\n            image_context = f\"Image shows: {image_caption}. Analysis: {classification_label}\"\n        \n        final_result = review_and_refine_answer(\n            initial_answer=initial_result,\n            original_question=english_question,\n            image_context=image_context,\n            conversation_context=conversation_context\n        )\n        \n        # NEW: Clean and translate final answer back to original language\n        cleaned_result = clean_answer(final_result)\n        \n        if original_lang != \"en\":\n            cleaned_result = translate_from_english(cleaned_result, original_lang)\n            print(f\"ğŸŒ Final answer translated back to {original_lang}\")\n        \n        # NEW: Add to conversation history with original language info\n        add_to_history(question_text, cleaned_result, image_context, original_lang)\n        \n        # Cleanup after each query\n        cleanup_memory()\n        \n        return f\"*LLM-Classified Agriculture Answer* ({original_lang.upper()}):\\n\\n{cleaned_result}\"\n            \n    except Exception as e:\n        cleanup_memory()\n        error_response = f\"âŒ Error: {str(e)}\"\n        \n        # NEW: Translate error response if needed\n        if 'original_lang' in locals() and original_lang != \"en\":\n            error_response = translate_from_english(error_response, original_lang)\n        \n        # Add error to history too\n        if 'question_text' in locals():\n            add_to_history(question_text, error_response, \"\", original_lang if 'original_lang' in locals() else \"en\")\n        return error_response\n\n# NEW: Function to clear conversation history\ndef clear_conversation_history():\n    \"\"\"Clear the conversation history\"\"\"\n    global conversation_history\n    conversation_history = []\n    return \"ğŸ—‘ Conversation history cleared! Starting fresh.\"\n\n# Initialize optimized system\nprint(\"ğŸš€ Initializing OPTIMIZED AgroRAG system with PURE LLM Classification, Conversational Memory, and Multilingual Support...\")\ninit_status = initialize_system()\nprint(init_status)\n\n# Streamlined Gradio interface for speed - MODIFIED to include multilingual support and conversation history management\nwith gr.Blocks(title=\"ğŸš€ Fast AgroRAG Assistant - Pure LLM Classification + Memory + Multilingual\") as demo:\n    gr.Markdown(\"\"\"\n    # ğŸš€ Fast AgroRAG Assistant (Pure LLM Classification + Memory + ğŸŒ Multilingual)\n    \n    *New Features:*\n    - ğŸ¤ Speech Recognition using speech_recognition library\n    - ğŸ–¼ Optimized image processing with resizing\n    - ğŸ§  *PURE LLM-BASED CLASSIFICATION* (No keyword rules - everything decided by AI)\n    - âš¡ Flexible classification (allows if EITHER question OR image is agriculture-related)\n    - ğŸ”§ GPU memory management and cleanup\n    - ğŸ’­ *SESSION-BASED CONVERSATIONAL MEMORY* (Remembers previous exchanges!)\n    - ğŸ¤– *INTERACTIVE RESPONSES* (Responds to greetings and casual conversation)\n    - ğŸŒ **MULTILINGUAL SUPPORT** (Ask questions in any language - auto-detected and translated!)\n    \n    ## ğŸŒ Multilingual Features:\n    - **Auto Language Detection**: Automatically detects your language\n    - **Real-time Translation**: Questions translated to English for processing, answers translated back\n    - **Supported Languages**: Spanish, French, German, Hindi, Chinese, Arabic, Portuguese, and many more!\n    - **Conversation Memory**: Remembers conversations in your original language\n    \n    ## New Interactive Features:\n    - Say \"Hi\", \"Hello\", \"Good morning\" in any language for friendly greetings\n    - Ask \"How are you?\" or \"What can you do?\" in your preferred language\n    - Use follow-up questions like \"Tell me more about this\" or \"Can you explain further?\"\n    - The bot remembers your conversation and can reference previous topics!\n    \n    ## Sample Questions to Try (in any language):\n    - English: \"What are the symptoms of tomato blight disease?\"\n    - Spanish: \"Â¿CuÃ¡les son los sÃ­ntomas de la enfermedad del tizÃ³n del tomate?\"\n    - French: \"Quels sont les symptÃ´mes de la maladie du mildiou de la tomate?\"\n    - Hindi: \"à¤Ÿà¤®à¤¾à¤Ÿà¤° à¤•à¥€ à¤à¥à¤²à¤¸à¤¾ à¤°à¥‹à¤— à¤•à¥‡ à¤²à¤•à¥à¤·à¤£ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚?\"\n    - German: \"Was sind die Symptome der TomatenfÃ¤ule?\"\n    \n    *Note:* All classification decisions are made by the Mistral 7B LLM with conversation and multilingual awareness!\n    \"\"\")\n    \n    # NEW: Add conversation history display and clear button\n    with gr.Row():\n        with gr.Column(scale=3):\n            pass  # Empty space for layout\n        with gr.Column(scale=1):\n            clear_history_btn = gr.Button(\"ğŸ—‘ Clear History\", variant=\"secondary\")\n    \n    with gr.Tab(\"ğŸ¤ Voice + Image + ğŸŒ Multilingual\"):        \n        with gr.Row():\n            with gr.Column():\n                audio_input = gr.Audio(label=\"ğŸ¤ Record Question (Any Language)\", type=\"numpy\")\n                transcription_output = gr.Textbox(label=\"ğŸ“ Transcribed Text\", lines=2, interactive=True)\n                image_input_voice = gr.Image(label=\"ğŸ“¸ Upload Image (optional)\", type=\"pil\")\n                submit_button = gr.Button(\"ğŸš€ Get Answer\", variant=\"primary\")\n            \n            with gr.Column():\n                answer_output = gr.Textbox(label=\"ğŸŒ¾ Answer\", lines=8)\n        \n        def transcribe_only(audio):\n            return transcribe_audio(audio) if audio else \"\"\n        \n        def get_answer(question_text, image):\n            return answer_question(question_text, image)\n        \n        audio_input.change(fn=transcribe_only, inputs=[audio_input], outputs=[transcription_output])\n        submit_button.click(fn=get_answer, inputs=[transcription_output, image_input_voice], outputs=[answer_output])\n    \n    with gr.Tab(\"ğŸ“ Text + Image + ğŸŒ Multilingual\"):\n        with gr.Row():\n            with gr.Column():\n                text_input = gr.Textbox(\n                    label=\"Ask about Agriculture in ANY Language! (Auto-detected)\", \n                    placeholder=\"Â¡Hola! / Bonjour! / à¤¨à¤®à¤¸à¥à¤¤à¥‡! / What disease is this? / Tell me more about organic farming\", \n                    lines=2\n                )\n                \n                # Sample questions buttons - MODIFIED to include multilingual examples\n                gr.Markdown(\"*Quick Sample Questions (Multiple Languages):*\")\n                with gr.Row():\n                    sample_btn1 = gr.Button(\"ğŸ‘‹ Hi there! (EN)\", size=\"sm\")\n                    sample_btn2 = gr.Button(\"ğŸŒ± Â¿RotaciÃ³n de cultivos? (ES)\", size=\"sm\")\n                    sample_btn3 = gr.Button(\"ğŸ’§ à¤¸à¤¿à¤‚à¤šà¤¾à¤ˆ à¤¤à¤•à¤¨à¥€à¤•? (HI)\", size=\"sm\")\n                \n                image_input = gr.Image(label=\"ğŸ“¸ Upload Plant Image\", type=\"pil\")\n                submit_text_button = gr.Button(\"ğŸš€ Get Answer\", variant=\"primary\")\n            \n            with gr.Column():\n                text_output = gr.Textbox(label=\"ğŸŒ¾ Answer\", lines=8)\n        \n        # NEW: Functions for multilingual sample buttons\n        def set_sample_text1():\n            return \"Hi there! How are you?\"\n        \n        def set_sample_text2():\n            return \"Â¿QuÃ© es la rotaciÃ³n de cultivos y cÃ³mo ayuda?\"\n        \n        def set_sample_text3():\n            return \"à¤¸à¤¬à¥à¤œà¤¿à¤¯à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤¸à¤¬à¤¸à¥‡ à¤…à¤šà¥à¤›à¥€ à¤¸à¤¿à¤‚à¤šà¤¾à¤ˆ à¤¤à¤•à¤¨à¥€à¤• à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚?\"\n        \n        # NEW: Connect sample buttons\n        sample_btn1.click(fn=set_sample_text1, outputs=[text_input])\n        sample_btn2.click(fn=set_sample_text2, outputs=[text_input])\n        sample_btn3.click(fn=set_sample_text3, outputs=[text_input])\n        \n        # Add the click event handler for the text submit button\n        submit_text_button.click(\n            fn=answer_question, \n            inputs=[text_input, image_input], \n            outputs=[text_output]\n        )\n        \n        # Examples section - MODIFIED to include multilingual examples\n        examples = gr.Examples(\n            examples=[\n                [\"Hello! How can you help me? ğŸ‡ºğŸ‡¸\"],\n                [\"Â¿QuÃ© es la rotaciÃ³n de cultivos? ğŸ‡ªğŸ‡¸\"],\n                [\"Comment fonctionne l'agriculture biologique? ğŸ‡«ğŸ‡·\"],\n                [\"à¤œà¥ˆà¤µà¤¿à¤• à¤–à¥‡à¤¤à¥€ à¤•à¥‡ à¤•à¥à¤¯à¤¾ à¤«à¤¾à¤¯à¤¦à¥‡ à¤¹à¥ˆà¤‚? ğŸ‡®ğŸ‡³\"],\n                [\"Was sind die Vorteile der TrÃ¶pfchenbewÃ¤sserung? ğŸ‡©ğŸ‡ª\"],\n                [\"Qual Ã© a importÃ¢ncia do pH do solo? ğŸ‡§ğŸ‡·\"],\n                [\"è°¢è°¢ä½ çš„å¸®åŠ©! ğŸ‡¨ğŸ‡³\"],\n                [\"Can you tell me more about this? ğŸ‡ºğŸ‡¸\"],  # Follow-up example\n            ],\n            inputs=[text_input]\n        )\n    \n    with gr.Tab(\"ğŸ“Š Conversation History + ğŸŒ Languages\"):\n        gr.Markdown(\"\"\"\n        ## ğŸ’­ Your Multilingual Conversation with AgroRAG\n        \n        This tab shows your recent conversation history in all languages. The bot remembers your previous questions and can reference them in follow-up questions.\n        \n        *Tips for using multilingual conversational memory:*\n        - After asking about a topic in any language, you can say \"Tell me more about this\" in the same or different language\n        - Use phrases like \"Can you explain that further?\" in your preferred language\n        - The bot will understand what \"this\" and \"that\" refer to based on previous conversation\n        - Switch languages anytime - the bot maintains context across languages!\n        \n        **Language Coverage**: English, Spanish, French, German, Hindi, Chinese, Arabic, Portuguese, Italian, Russian, Japanese, Korean, and many more!\n        \"\"\")\n        \n        history_display = gr.JSON(\n            label=\"Recent Multilingual Conversations\",\n            value=lambda: conversation_history[-5:] if conversation_history else []  # Show last 5\n        )\n        \n        refresh_history_btn = gr.Button(\"ğŸ”„ Refresh History\", variant=\"secondary\")\n        \n        def refresh_history():\n            return conversation_history[-5:] if conversation_history else []\n        \n        refresh_history_btn.click(fn=refresh_history, outputs=[history_display])\n    \n    # NEW: Connect clear history button\n    clear_history_btn.click(fn=clear_conversation_history, outputs=[])\n        \n\nif __name__ == \"__main__\":\n    # Install required packages if not available\n    try:\n        import speech_recognition as sr\n    except ImportError:\n        os.system(\"pip install SpeechRecognition\")\n        import speech_recognition as sr\n    \n    try:\n        import pocketsphinx\n    except ImportError:\n        os.system(\"pip install pocketsphinx\")\n    \n    # NEW: Install multilingual packages\n    try:\n        from deep_translator import GoogleTranslator\n    except ImportError:\n        os.system(\"pip install deep-translator\")\n        from deep_translator import GoogleTranslator\n    \n    try:\n        from langdetect import detect\n    except ImportError:\n        os.system(\"pip install langdetect\")\n        from langdetect import detect\n    \n    try:\n        demo.launch(share=True, server_name=\"0.0.0.0\")\n    except OSError as e:\n        if \"Cannot find empty port\" in str(e):\n            demo.launch(share=True)\n        else:\n            raise e","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T06:39:45.290166Z","iopub.execute_input":"2025-08-19T06:39:45.290843Z","iopub.status.idle":"2025-08-19T06:43:20.322291Z","shell.execute_reply.started":"2025-08-19T06:39:45.290814Z","shell.execute_reply":"2025-08-19T06:43:20.321555Z"}},"outputs":[{"name":"stderr","text":"2025-08-19 06:40:00.158049: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755585600.336514      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755585600.388213      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"name":"stdout","text":"ğŸš€ Initializing OPTIMIZED AgroRAG system with PURE LLM Classification, Conversational Memory, and Multilingual Support...\nInitializing speech recognition...\nâœ… Speech Recognition initialized successfully\nInitializing optimized image models...\nLoading optimized BLIP model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e16e6500bfda4f9194369302578741dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68803044d5304e1aa95362d2f18f6e6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff2c7a3bdaf24ca9bd6e791d71f84c89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f95bd1b1fbf4a16a27600f5c7d9cae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9651b70715eb4bf484a20f1d727b92b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c7b668a435a4ccbb0e80f212b5eeb16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f65860818c5b4ab794d38db8f833c93f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"983364eac9df48b5bf38813134dc4a49"}},"metadata":{}},{"name":"stdout","text":"âœ… BLIP model loaded and optimized\nLoading crop disease ViT model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/325 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99ed90c1443a40f1bb55aef527aa8ce9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5d9c88ceab04385be66b8995e32862c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/22.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abef8501de7c485597dfd4e82d941537"}},"metadata":{}},{"name":"stdout","text":"âœ… ViT model loaded and optimized\nInitializing LLM-based classifier...\nâœ… Using main Mistral 7B model for LLM-based classification (shared with QA)\nLoading optimized FAISS index...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/4284124900.py:564: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embedder = HuggingFaceEmbeddings(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbdb78c7efb64f758abd2b77317106cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f73ffed384e9414faead624043c7616c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf681429abe14550b95c52ac4ddd3927"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"545b85aa869b47ceb8106f41ba014fad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44920259243a47089cf11ad13ede0430"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cf852dff07e45dc8a4bda5a20be2fbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f4270e854e4465ca9bc7472e5674f68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45b10758af474dd385893572d356387a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf8b79d1390e43b2a5da6b219740c143"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10bc16d1a4a24d898210c73a5f2d630a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05aa9f38d983499cbcd5bdf06bcde113"}},"metadata":{}},{"name":"stdout","text":"âœ… FAISS vectorstore loaded with GPU embeddings\nLoading optimized Mistral model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d741a5dd854740ea8ee49beefb8ac136"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eee27195e8b0405a8625fb43523eb023"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16e9d3a66ee54da28585b0f9ed4f4975"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f18c51f5a5854609a0fcb6b3325b647e"}},"metadata":{}},{"name":"stdout","text":"Loading Mistral model with optimizations...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bca0d91377c34d0f86f3b9f589beecee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87e5053994de4747af18548b4c871f06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"196e46b43fa64246bae8da85cb409686"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c771ab958bf4e45b247903a05d497d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5e08484da284beea0db983fd90489d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"424cea13b99b4b17be306c1663704b15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9986b3521e843b7865afc73b7646f14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7390e27e815d4819bc4d3d6e004180d6"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n/tmp/ipykernel_36/4284124900.py:610: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n  llm = HuggingFacePipeline(pipeline=pipe)\n","output_type":"stream"},{"name":"stdout","text":"ğŸš€ OPTIMIZED System ready! Speech recognition, image analysis, conversational memory, interactive responses, and multilingual support initialized.\nCollecting pocketsphinx\n  Downloading pocketsphinx-5.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting sounddevice (from pocketsphinx)\n  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice->pocketsphinx) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice->pocketsphinx) (2.22)\nDownloading pocketsphinx-5.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.2 MB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 29.2/29.2 MB 40.1 MB/s eta 0:00:00\nDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\nInstalling collected packages: sounddevice, pocketsphinx\nSuccessfully installed pocketsphinx-5.0.4 sounddevice-0.5.2\n* Running on local URL:  http://0.0.0.0:7860\n* Running on public URL: https://d6525813249fd8b13e.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://d6525813249fd8b13e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"ğŸŒ Detected language: sw, translated to English for processing\nğŸ¯ Interactive input detected\nğŸŒ Interactive response translated back to sw\nğŸŒ Detected language: fi, translated to English for processing\nğŸ¯ Interactive input detected\nğŸŒ Interactive response translated back to fi\nğŸ¯ Interactive input detected\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"ğŸŒ Detected language: hi, translated to English for processing\nğŸ”— Relates to conversation history: False\nğŸ§  Using LLM to classify: 'What are the best irrigation techniques for vegeta...'\nğŸ¤– LLM Classification result: YES. THE QUESTION\nâœ… Final classification: True\nğŸ¤– LLM Text-only classification: True\nğŸ¤– Generating initial answer from RAG system...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/4284124900.py:863: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  initial_result = qa_chain.run(full_context)\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ” Reviewing and refining the answer...\nğŸ” Reviewing and refining the initial answer...\nâœ… Answer successfully reviewed and refined\nğŸŒ Final answer translated back to hi\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ”— Relates to conversation history: False\nğŸ§  Using LLM to classify: 'what is crop rotation...'\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ¤– LLM Classification result: YES. CROP\nâœ… Final classification: True\nğŸ¤– LLM Text-only classification: True\nğŸ¤– Generating initial answer from RAG system...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ” Reviewing and refining the answer...\nğŸ” Reviewing and refining the initial answer...\nâœ… Answer successfully reviewed and refined\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}